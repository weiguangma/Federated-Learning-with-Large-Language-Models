#!/usr/bin/env python3
# /********************************************************************************
#  * @Author: zhangqiuhong
#  * @Date: 2025-09-20
#  * @Description: è”é‚¦å­¦ä¹ Qwenæ¨¡å‹è¯„ä¼°è„šæœ¬ - ç»Ÿä¸€è¯„ä¼°é€»è¾‘
#  ********************************************************************************/

import torch
import json
import logging
import argparse
from pathlib import Path
from tqdm import tqdm
from typing import Dict, List, Tuple
import re
from collections import Counter
from federated_model import FederatedQwenSystem

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class FederatedModelEvaluator:
    """è”é‚¦æ¨¡å‹è¯„ä¼°å™¨"""
    
    def __init__(self, model_path: str, checkpoint_path: str, device: str = 'cuda'):
        self.model_path = model_path
        self.checkpoint_path = checkpoint_path
        self.device = device
        
        # åŠ è½½æ¨¡å‹
        logger.info("ğŸ”§ åŠ è½½è”é‚¦Qwenæ¨¡å‹...")
        self.model = FederatedQwenSystem(model_path=model_path, device=device)
        
        # åŠ è½½æ£€æŸ¥ç‚¹
        if Path(checkpoint_path).exists():
            logger.info(f"ğŸ“¥ åŠ è½½æ£€æŸ¥ç‚¹: {checkpoint_path}")
            checkpoint = torch.load(checkpoint_path, map_location=device)
            
            # å…¼å®¹ä¸¤ç§ä¿å­˜æ ¼å¼
            if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
                # æ ‡å‡†æ ¼å¼ï¼šåŒ…å«model_state_dicté”®çš„å­—å…¸
                self.model.load_state_dict(checkpoint['model_state_dict'])
                logger.info("âœ… æ¨¡å‹æ£€æŸ¥ç‚¹åŠ è½½æˆåŠŸï¼ˆæ ‡å‡†æ ¼å¼ï¼‰")
            else:
                # ç®€å•æ ¼å¼ï¼šç›´æ¥æ˜¯state_dict
                self.model.load_state_dict(checkpoint)
                logger.info("âœ… æ¨¡å‹æ£€æŸ¥ç‚¹åŠ è½½æˆåŠŸï¼ˆç®€å•æ ¼å¼ï¼‰")
        else:
            logger.warning(f"âš ï¸  æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
        
        self.model.to(device)
        self.model.eval()
        
        logger.info("âœ… è”é‚¦æ¨¡å‹è¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def load_test_data(self, data_file: Path, max_samples: int = None) -> List[Dict]:
        """åŠ è½½æµ‹è¯•æ•°æ®"""
        logger.info(f"ğŸ“Š ä» {data_file} åŠ è½½æµ‹è¯•æ•°æ®...")
        
        samples = []
        with open(data_file, 'r', encoding='utf-8') as f:
            for line_idx, line in enumerate(f):
                if max_samples and len(samples) >= max_samples:
                    break
                    
                try:
                    sample = json.loads(line.strip())
                    samples.append(sample)
                except json.JSONDecodeError as e:
                    logger.warning(f"è·³è¿‡æ— æ•ˆJSONè¡Œ {line_idx}: {e}")
                    continue
        
        logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(samples)} ä¸ªæµ‹è¯•æ ·æœ¬")
        return samples
    
    def parse_qwen_sample(self, sample: dict) -> Tuple[str, Dict[str, str], str]:
        """è§£ææ ‡å‡†Qwenæ ¼å¼çš„æ•°æ®"""
        full_instruction = sample['instruction']
        
        # ä»instructionä¸­æå–æœåŠ¡ç«¯å’Œå®¢æˆ·ç«¯æŒ‡ä»¤
        parts = full_instruction.split('<|object_ref_start|>')
        server_instruction = parts[0].strip()
        
        # æå–å®¢æˆ·ç«¯æŒ‡ä»¤
        client_instructions = {}
        for i in range(1, min(4, len(parts))):  # æœ€å¤š3ä¸ªå®¢æˆ·ç«¯
            client_part = parts[i]
            if '<|object_ref_end|>' in client_part:
                client_content = client_part.split('<|object_ref_end|>')[0].strip()
                client_instructions[f'client_{i:02d}'] = client_content
        
        # ç¡®ä¿æœ‰3ä¸ªå®¢æˆ·ç«¯æŒ‡ä»¤
        for i in range(1, 4):
            key = f'client_{i:02d}'
            if key not in client_instructions:
                client_instructions[key] = "æ— é¢å¤–ä¿¡æ¯"
        
        return server_instruction, client_instructions, sample['output']
    
    def extract_transport_mode(self, text: str) -> str:
        """ä»æ–‡æœ¬ä¸­æå–è¿è¾“æ–¹å¼"""
        transport_patterns = [
            r'è¿è¾“æ–¹å¼é€‰æ‹©[ï¼š:]\s*([^*\n]+)',
            r'é€‰æ‹©\s*([^*\n]*è¿è¾“[^*\n]*)',
            r'å»ºè®®.*?([^*\n]*è¿è¾“[^*\n]*)',
            r'(é“è·¯è¿è¾“|å…¬è·¯è¿è¾“|æµ·è¿|ç©ºè¿|æ°´è¿|å¤šå¼è”è¿)'
        ]
        
        for pattern in transport_patterns:
            match = re.search(pattern, text)
            if match:
                return match.group(1).strip()
        
        return "æœªçŸ¥"
    
    def extract_destination(self, text: str) -> Tuple[str, str]:
        """ä»æ–‡æœ¬ä¸­æå–ç›®çš„åœ°åŸå¸‚å’Œçœä»½"""
        # ç›®çš„åœ°åŸå¸‚
        city_patterns = [
            r'ç›®çš„åœ°åŸå¸‚[ï¼š:]\s*([^*\n]+)',
            r'ç›®çš„åœ°[ï¼š:]?\s*([^*\n]*[å¸‚å¿åŒº][^*\n]*)',
            r'åˆ°è¾¾\s*([^*\n]*[å¸‚å¿åŒº][^*\n]*)'
        ]
        
        city = "æœªçŸ¥"
        for pattern in city_patterns:
            match = re.search(pattern, text)
            if match:
                city = match.group(1).strip()
                break
        
        # ç›®çš„åœ°çœä»½
        province_patterns = [
            r'ç›®çš„åœ°çœä»½[ï¼š:]\s*([^*\n]+)',
            r'([^*\n]*[çœå¸‚åŒº][^*\n]*)',
        ]
        
        province = "æœªçŸ¥"
        for pattern in province_patterns:
            match = re.search(pattern, text)
            if match:
                province = match.group(1).strip()
                break
        
        return city, province
    
    def calculate_metrics(self, predictions: List[str], targets: List[str]) -> Dict[str, float]:
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        if len(predictions) != len(targets):
            logger.error("é¢„æµ‹ç»“æœå’Œç›®æ ‡ç»“æœæ•°é‡ä¸åŒ¹é…")
            return {}
        
        # æå–ç»“æ„åŒ–ä¿¡æ¯
        pred_transports = [self.extract_transport_mode(pred) for pred in predictions]
        target_transports = [self.extract_transport_mode(target) for target in targets]
        
        pred_cities = []
        pred_provinces = []
        target_cities = []
        target_provinces = []
        
        for pred, target in zip(predictions, targets):
            pred_city, pred_province = self.extract_destination(pred)
            target_city, target_province = self.extract_destination(target)
            
            pred_cities.append(pred_city)
            pred_provinces.append(pred_province)
            target_cities.append(target_city)
            target_provinces.append(target_province)
        
        # è®¡ç®—å‡†ç¡®ç‡
        transport_acc = sum(1 for p, t in zip(pred_transports, target_transports) if p == t) / len(predictions)
        city_acc = sum(1 for p, t in zip(pred_cities, target_cities) if p == t) / len(predictions)
        province_acc = sum(1 for p, t in zip(pred_provinces, target_provinces) if p == t) / len(predictions)
        
        # è®¡ç®—BLEUåˆ†æ•°ï¼ˆç®€åŒ–ç‰ˆï¼‰
        def simple_bleu(pred: str, target: str) -> float:
            pred_words = set(pred.split())
            target_words = set(target.split())
            if len(target_words) == 0:
                return 0.0
            intersection = pred_words.intersection(target_words)
            return len(intersection) / len(target_words)
        
        bleu_scores = [simple_bleu(pred, target) for pred, target in zip(predictions, targets)]
        avg_bleu = sum(bleu_scores) / len(bleu_scores)
        
        # è®¡ç®—å®Œæ•´åŒ¹é…å‡†ç¡®ç‡
        exact_match = sum(1 for p, t in zip(predictions, targets) if p.strip() == t.strip()) / len(predictions)
        
        metrics = {
            'transport_accuracy': transport_acc,
            'city_accuracy': city_acc,
            'province_accuracy': province_acc,
            'avg_bleu_score': avg_bleu,
            'exact_match_accuracy': exact_match,
            'total_samples': len(predictions)
        }
        
        return metrics
    
    def evaluate_model(self, test_data: List[Dict], max_new_tokens: int = 200, 
                      temperature: float = 0.7, batch_size: int = 1) -> Dict:
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        logger.info("ğŸ§ª å¼€å§‹æ¨¡å‹è¯„ä¼°...")
        
        predictions = []
        targets = []
        generation_errors = 0
        
        with torch.no_grad():
            for i, sample in enumerate(tqdm(test_data, desc="è¯„ä¼°è¿›åº¦")):
                try:
                    # è§£ææ ·æœ¬
                    server_instruction, client_instructions, target_output = self.parse_qwen_sample(sample)
                    
                    # ç”Ÿæˆé¢„æµ‹
                    generated_text = self.model.generate(
                        server_instruction,
                        client_instructions,
                        max_new_tokens=max_new_tokens,
                        temperature=temperature,
                        do_sample=True
                    )
                    
                    predictions.append(generated_text)
                    targets.append(target_output)
                    
                except Exception as e:
                    logger.warning(f"æ ·æœ¬ {i} ç”Ÿæˆå¤±è´¥: {e}")
                    generation_errors += 1
                    predictions.append("ç”Ÿæˆå¤±è´¥")
                    targets.append(sample.get('output', ''))
        
        # è®¡ç®—æŒ‡æ ‡
        logger.info("ğŸ“Š è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
        metrics = self.calculate_metrics(predictions, targets)
        metrics['generation_errors'] = generation_errors
        metrics['generation_success_rate'] = 1.0 - (generation_errors / len(test_data))
        
        return {
            'metrics': metrics,
            'predictions': predictions,
            'targets': targets
        }
    
    def print_evaluation_report(self, results: Dict):
        """æ‰“å°è¯„ä¼°æŠ¥å‘Š"""
        metrics = results['metrics']
        predictions = results['predictions']
        targets = results['targets']
        
        logger.info("=" * 80)
        logger.info("ğŸ“ˆ è”é‚¦Qwenæ¨¡å‹è¯„ä¼°æŠ¥å‘Š")
        logger.info("=" * 80)
        
        logger.info(f"ğŸ“Š åŸºç¡€æŒ‡æ ‡:")
        logger.info(f"   - æ€»æ ·æœ¬æ•°: {metrics['total_samples']}")
        logger.info(f"   - ç”ŸæˆæˆåŠŸç‡: {metrics['generation_success_rate']:.2%}")
        logger.info(f"   - ç”Ÿæˆé”™è¯¯æ•°: {metrics['generation_errors']}")
        
        logger.info(f"ğŸ¯ ä»»åŠ¡å‡†ç¡®ç‡:")
        logger.info(f"   - è¿è¾“æ–¹å¼å‡†ç¡®ç‡: {metrics['transport_accuracy']:.2%}")
        logger.info(f"   - ç›®çš„åœ°åŸå¸‚å‡†ç¡®ç‡: {metrics['city_accuracy']:.2%}")
        logger.info(f"   - ç›®çš„åœ°çœä»½å‡†ç¡®ç‡: {metrics['province_accuracy']:.2%}")
        
        logger.info(f"ğŸ“ æ–‡æœ¬è´¨é‡æŒ‡æ ‡:")
        logger.info(f"   - å¹³å‡BLEUåˆ†æ•°: {metrics['avg_bleu_score']:.4f}")
        logger.info(f"   - å®Œå…¨åŒ¹é…å‡†ç¡®ç‡: {metrics['exact_match_accuracy']:.2%}")
        
        # æ˜¾ç¤ºä¸€äº›ç¤ºä¾‹
        logger.info(f"ğŸ” ç”Ÿæˆç¤ºä¾‹ (å‰3ä¸ª):")
        for i in range(min(3, len(predictions))):
            logger.info(f"   æ ·æœ¬ {i+1}:")
            logger.info(f"     ç›®æ ‡: {targets[i][:100]}...")
            logger.info(f"     é¢„æµ‹: {predictions[i][:100]}...")
            logger.info(f"     ---")
        
        logger.info("=" * 80)
    
    def save_evaluation_results(self, results: Dict, output_file: str):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        logger.info(f"ğŸ’¾ ä¿å­˜è¯„ä¼°ç»“æœåˆ°: {output_file}")
        
        # å‡†å¤‡ä¿å­˜æ•°æ®
        save_data = {
            'metrics': results['metrics'],
            'model_path': self.model_path,
            'checkpoint_path': self.checkpoint_path,
            'evaluation_samples': []
        }
        
        # æ·»åŠ æ ·æœ¬è¯¦æƒ…ï¼ˆé™åˆ¶æ•°é‡ä»¥é¿å…æ–‡ä»¶è¿‡å¤§ï¼‰
        max_save_samples = min(100, len(results['predictions']))
        for i in range(max_save_samples):
            save_data['evaluation_samples'].append({
                'sample_id': i,
                'prediction': results['predictions'][i],
                'target': results['targets'][i]
            })
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2)
        
        logger.info("âœ… è¯„ä¼°ç»“æœä¿å­˜å®Œæˆ")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="è¯„ä¼°è”é‚¦Qwenæ¨¡å‹")
    parser.add_argument("--model_path", type=str,
                       default="/root/autodl-tmp/Federated_learning/llm_init_ckpt/Qwen2.5-0.5B-Instruct",
                       help="é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„")
    parser.add_argument("--checkpoint_path", type=str,
                       default="federated_qwen_output/federated_qwen_model.pth",
                       help="è®­ç»ƒåçš„æ£€æŸ¥ç‚¹è·¯å¾„")
    parser.add_argument("--test_data", type=str,
                       default="./data/qwen_processed/qwen_federated_train.jsonl",
                       help="æµ‹è¯•æ•°æ®æ–‡ä»¶")
    parser.add_argument("--max_samples", type=int, default=100,
                       help="æœ€å¤§æµ‹è¯•æ ·æœ¬æ•°")
    parser.add_argument("--max_new_tokens", type=int, default=200,
                       help="ç”Ÿæˆçš„æœ€å¤§tokenæ•°")
    parser.add_argument("--temperature", type=float, default=0.7,
                       help="ç”Ÿæˆæ¸©åº¦")
    parser.add_argument("--output_file", type=str, default="evaluation_results.json",
                       help="è¯„ä¼°ç»“æœè¾“å‡ºæ–‡ä»¶")
    parser.add_argument("--device", type=str, default="cuda",
                       help="è®¡ç®—è®¾å¤‡")
    
    args = parser.parse_args()
    
    try:
        # åˆå§‹åŒ–è¯„ä¼°å™¨
        evaluator = FederatedModelEvaluator(
            model_path=args.model_path,
            checkpoint_path=args.checkpoint_path,
            device=args.device
        )
        
        # åŠ è½½æµ‹è¯•æ•°æ®
        test_data_file = Path(args.test_data)
        if not test_data_file.exists():
            logger.error(f"æµ‹è¯•æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {test_data_file}")
            return False
        
        test_data = evaluator.load_test_data(test_data_file, args.max_samples)
        
        if len(test_data) == 0:
            logger.error("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æµ‹è¯•æ•°æ®")
            return False
        
        # æ‰§è¡Œè¯„ä¼°
        results = evaluator.evaluate_model(
            test_data=test_data,
            max_new_tokens=args.max_new_tokens,
            temperature=args.temperature
        )
        
        # æ‰“å°æŠ¥å‘Š
        evaluator.print_evaluation_report(results)
        
        # ä¿å­˜ç»“æœ
        evaluator.save_evaluation_results(results, args.output_file)
        
        logger.info("ğŸ‰ æ¨¡å‹è¯„ä¼°å®Œæˆï¼")
        return True
        
    except Exception as e:
        logger.error(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    if not success:
        exit(1)