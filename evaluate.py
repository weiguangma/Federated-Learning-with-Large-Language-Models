#!/usr/bin/env python3
# /********************************************************************************
#  * @Author: zhangqiuhong
#  * @Date: 2025-09-20
#  * @Description: è”é‚¦å­¦ä¹ Qwenæ¨¡å‹è¯„ä¼°è„šæœ¬ - ç»Ÿä¸€è¯„ä¼°é€»è¾‘
#  ********************************************************************************/

import torch
import json
import logging
import argparse
from pathlib import Path
from tqdm import tqdm
from typing import Dict, List, Tuple
import re
from collections import Counter, defaultdict
import numpy as np
from federated_model import FederatedQwenSystem

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class FederatedModelEvaluator:
    """è”é‚¦æ¨¡å‹è¯„ä¼°å™¨"""
    
    def __init__(self, model_path: str, checkpoint_path: str, device: str = 'cuda'):
        self.model_path = model_path
        self.checkpoint_path = checkpoint_path
        self.device = device
        
        # åŠ è½½æ¨¡å‹
        logger.info("ğŸ”§ åŠ è½½è”é‚¦Qwenæ¨¡å‹...")
        self.model = FederatedQwenSystem(model_path=model_path, device=device)
        
        # åŠ è½½æ£€æŸ¥ç‚¹
        if Path(checkpoint_path).exists():
            logger.info(f"ğŸ“¥ åŠ è½½æ£€æŸ¥ç‚¹: {checkpoint_path}")
            checkpoint = torch.load(checkpoint_path, map_location=device)
            
            # å…¼å®¹ä¸¤ç§ä¿å­˜æ ¼å¼
            if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
                # æ ‡å‡†æ ¼å¼ï¼šåŒ…å«model_state_dicté”®çš„å­—å…¸
                self.model.load_state_dict(checkpoint['model_state_dict'])
                logger.info("âœ… æ¨¡å‹æ£€æŸ¥ç‚¹åŠ è½½æˆåŠŸï¼ˆæ ‡å‡†æ ¼å¼ï¼‰")
            else:
                # ç®€å•æ ¼å¼ï¼šç›´æ¥æ˜¯state_dict
                self.model.load_state_dict(checkpoint)
                logger.info("âœ… æ¨¡å‹æ£€æŸ¥ç‚¹åŠ è½½æˆåŠŸï¼ˆç®€å•æ ¼å¼ï¼‰")
        else:
            logger.warning(f"âš ï¸  æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
        
        self.model.to(device)
        self.model.eval()
        
        logger.info("âœ… è”é‚¦æ¨¡å‹è¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def load_test_data(self, data_file: Path, max_samples: int = None) -> List[Dict]:
        """åŠ è½½æµ‹è¯•æ•°æ®"""
        logger.info(f"ğŸ“Š ä» {data_file} åŠ è½½æµ‹è¯•æ•°æ®...")
        
        samples = []
        with open(data_file, 'r', encoding='utf-8') as f:
            for line_idx, line in enumerate(f):
                if max_samples and len(samples) >= max_samples:
                    break
                    
                try:
                    sample = json.loads(line.strip())
                    samples.append(sample)
                except json.JSONDecodeError as e:
                    logger.warning(f"è·³è¿‡æ— æ•ˆJSONè¡Œ {line_idx}: {e}")
                    continue
        
        logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(samples)} ä¸ªæµ‹è¯•æ ·æœ¬")
        return samples
    
    def parse_qwen_sample(self, sample: dict) -> Tuple[str, Dict[str, str], str]:
        """è§£ææ ‡å‡†Qwenæ ¼å¼çš„æ•°æ®"""
        full_instruction = sample['instruction']
        
        # ä»instructionä¸­æå–æœåŠ¡ç«¯å’Œå®¢æˆ·ç«¯æŒ‡ä»¤
        parts = full_instruction.split('<|object_ref_start|>')
        server_instruction = parts[0].strip()
        
        # æå–å®¢æˆ·ç«¯æŒ‡ä»¤
        client_instructions = {}
        for i in range(1, min(4, len(parts))):  # æœ€å¤š3ä¸ªå®¢æˆ·ç«¯
            client_part = parts[i]
            if '<|object_ref_end|>' in client_part:
                client_content = client_part.split('<|object_ref_end|>')[0].strip()
                client_instructions[f'client_{i:02d}'] = client_content
        
        # ç¡®ä¿æœ‰3ä¸ªå®¢æˆ·ç«¯æŒ‡ä»¤
        for i in range(1, 4):
            key = f'client_{i:02d}'
            if key not in client_instructions:
                client_instructions[key] = "æ— é¢å¤–ä¿¡æ¯"
        
        return server_instruction, client_instructions, sample['output']
    
    def extract_transport_mode(self, text: str) -> str:
        """ä»æ–‡æœ¬ä¸­æå–è¿è¾“æ–¹å¼"""
        transport_patterns = [
            r'è¿è¾“æ–¹å¼é€‰æ‹©[ï¼š:]\s*([^*\n]+)',
            r'é€‰æ‹©\s*([^*\n]*è¿è¾“[^*\n]*)',
            r'å»ºè®®.*?([^*\n]*è¿è¾“[^*\n]*)',
            r'(é“è·¯è¿è¾“|å…¬è·¯è¿è¾“|æµ·è¿|ç©ºè¿|æ°´è¿|å¤šå¼è”è¿)'
        ]
        
        for pattern in transport_patterns:
            match = re.search(pattern, text)
            if match:
                return match.group(1).strip()
        
        return "æœªçŸ¥"
    
    def extract_destination(self, text: str) -> Tuple[str, str]:
        """ä»æ–‡æœ¬ä¸­æå–ç›®çš„åœ°åŸå¸‚å’Œçœä»½"""
        # ç›®çš„åœ°åŸå¸‚
        city_patterns = [
            r'ç›®çš„åœ°åŸå¸‚[ï¼š:]\s*([^*\n]+)',
            r'ç›®çš„åœ°[ï¼š:]?\s*([^*\n]*[å¸‚å¿åŒº][^*\n]*)',
            r'åˆ°è¾¾\s*([^*\n]*[å¸‚å¿åŒº][^*\n]*)'
        ]
        
        city = "æœªçŸ¥"
        for pattern in city_patterns:
            match = re.search(pattern, text)
            if match:
                city = match.group(1).strip()
                break
        
        # ç›®çš„åœ°çœä»½
        province_patterns = [
            r'ç›®çš„åœ°çœä»½[ï¼š:]\s*([^*\n]+)',
            r'([^*\n]*[çœå¸‚åŒº][^*\n]*)',
        ]
        
        province = "æœªçŸ¥"
        for pattern in province_patterns:
            match = re.search(pattern, text)
            if match:
                province = match.group(1).strip()
                break
        
        return city, province
    
    def calculate_classification_metrics(self, predictions: List[str], targets: List[str], task_name: str) -> Dict[str, float]:
        """è®¡ç®—åˆ†ç±»ä»»åŠ¡çš„è¯¦ç»†æŒ‡æ ‡ï¼ˆç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1å€¼ç­‰ï¼‰"""
        if len(predictions) != len(targets):
            return {}
        
        # è·å–æ‰€æœ‰å”¯ä¸€ç±»åˆ«
        all_labels = list(set(predictions + targets))
        if "æœªçŸ¥" in all_labels:
            all_labels.remove("æœªçŸ¥")  # ç§»é™¤æœªçŸ¥ç±»åˆ«
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„TP, FP, FN
        per_class_metrics = {}
        for label in all_labels:
            tp = sum(1 for p, t in zip(predictions, targets) if p == label and t == label)
            fp = sum(1 for p, t in zip(predictions, targets) if p == label and t != label)
            fn = sum(1 for p, t in zip(predictions, targets) if p != label and t == label)
            
            # è®¡ç®—ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1å€¼
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
            
            per_class_metrics[label] = {
                'precision': precision,
                'recall': recall,
                'f1': f1,
                'support': sum(1 for t in targets if t == label)
            }
        
        # è®¡ç®—macroå¹³å‡
        if all_labels:
            macro_precision = np.mean([metrics['precision'] for metrics in per_class_metrics.values()])
            macro_recall = np.mean([metrics['recall'] for metrics in per_class_metrics.values()])
            macro_f1 = np.mean([metrics['f1'] for metrics in per_class_metrics.values()])
        else:
            macro_precision = macro_recall = macro_f1 = 0.0
        
        # è®¡ç®—weightedå¹³å‡
        total_support = sum(metrics['support'] for metrics in per_class_metrics.values())
        if total_support > 0:
            weighted_precision = sum(metrics['precision'] * metrics['support'] for metrics in per_class_metrics.values()) / total_support
            weighted_recall = sum(metrics['recall'] * metrics['support'] for metrics in per_class_metrics.values()) / total_support
            weighted_f1 = sum(metrics['f1'] * metrics['support'] for metrics in per_class_metrics.values()) / total_support
        else:
            weighted_precision = weighted_recall = weighted_f1 = 0.0
        
        # è®¡ç®—æ•´ä½“å‡†ç¡®ç‡
        accuracy = sum(1 for p, t in zip(predictions, targets) if p == t) / len(predictions)
        
        return {
            f'{task_name}_accuracy': accuracy,
            f'{task_name}_macro_precision': macro_precision,
            f'{task_name}_macro_recall': macro_recall,
            f'{task_name}_macro_f1': macro_f1,
            f'{task_name}_weighted_precision': weighted_precision,
            f'{task_name}_weighted_recall': weighted_recall,
            f'{task_name}_weighted_f1': weighted_f1,
            f'{task_name}_per_class': per_class_metrics,
            f'{task_name}_label_distribution': Counter(targets)
        }
    
    def calculate_confusion_matrix(self, predictions: List[str], targets: List[str]) -> Dict[str, Dict[str, int]]:
        """è®¡ç®—æ··æ·†çŸ©é˜µ"""
        all_labels = sorted(list(set(predictions + targets)))
        confusion_matrix = defaultdict(lambda: defaultdict(int))
        
        for pred, target in zip(predictions, targets):
            confusion_matrix[target][pred] += 1
        
        return dict(confusion_matrix)
    
    def calculate_metrics(self, predictions: List[str], targets: List[str]) -> Dict[str, float]:
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡ - å¢å¼ºç‰ˆæœ¬"""
        if len(predictions) != len(targets):
            logger.error("é¢„æµ‹ç»“æœå’Œç›®æ ‡ç»“æœæ•°é‡ä¸åŒ¹é…")
            return {}
        
        # æå–ç»“æ„åŒ–ä¿¡æ¯
        pred_transports = [self.extract_transport_mode(pred) for pred in predictions]
        target_transports = [self.extract_transport_mode(target) for target in targets]
        
        pred_cities = []
        pred_provinces = []
        target_cities = []
        target_provinces = []
        
        for pred, target in zip(predictions, targets):
            pred_city, pred_province = self.extract_destination(pred)
            target_city, target_province = self.extract_destination(target)
            
            pred_cities.append(pred_city)
            pred_provinces.append(pred_province)
            target_cities.append(target_city)
            target_provinces.append(target_province)
        
        # è®¡ç®—å„ä»»åŠ¡çš„è¯¦ç»†åˆ†ç±»æŒ‡æ ‡
        transport_metrics = self.calculate_classification_metrics(pred_transports, target_transports, 'transport')
        city_metrics = self.calculate_classification_metrics(pred_cities, target_cities, 'city')
        province_metrics = self.calculate_classification_metrics(pred_provinces, target_provinces, 'province')
        
        # è®¡ç®—BLEUåˆ†æ•°ï¼ˆç®€åŒ–ç‰ˆï¼‰
        def simple_bleu(pred: str, target: str) -> float:
            pred_words = set(pred.split())
            target_words = set(target.split())
            if len(target_words) == 0:
                return 0.0
            intersection = pred_words.intersection(target_words)
            return len(intersection) / len(target_words)
        
        bleu_scores = [simple_bleu(pred, target) for pred, target in zip(predictions, targets)]
        avg_bleu = sum(bleu_scores) / len(bleu_scores)
        
        # è®¡ç®—å®Œæ•´åŒ¹é…å‡†ç¡®ç‡
        exact_match = sum(1 for p, t in zip(predictions, targets) if p.strip() == t.strip()) / len(predictions)
        
        # è®¡ç®—æ··æ·†çŸ©é˜µ
        transport_confusion = self.calculate_confusion_matrix(pred_transports, target_transports)
        city_confusion = self.calculate_confusion_matrix(pred_cities, target_cities)
        province_confusion = self.calculate_confusion_matrix(pred_provinces, target_provinces)
        
        # åˆå¹¶æ‰€æœ‰æŒ‡æ ‡
        all_metrics = {
            'total_samples': len(predictions),
            'avg_bleu_score': avg_bleu,
            'exact_match_accuracy': exact_match,
        }
        
        # æ·»åŠ å„ä»»åŠ¡çš„æŒ‡æ ‡
        all_metrics.update(transport_metrics)
        all_metrics.update(city_metrics)
        all_metrics.update(province_metrics)
        
        # æ·»åŠ æ··æ·†çŸ©é˜µ
        all_metrics['confusion_matrices'] = {
            'transport': transport_confusion,
            'city': city_confusion,
            'province': province_confusion
        }
        
        # ä¿æŒå‘åå…¼å®¹æ€§çš„ç®€åŒ–æŒ‡æ ‡
        all_metrics['transport_accuracy'] = transport_metrics.get('transport_accuracy', 0.0)
        all_metrics['city_accuracy'] = city_metrics.get('city_accuracy', 0.0)
        all_metrics['province_accuracy'] = province_metrics.get('province_accuracy', 0.0)
        
        return all_metrics
    
    def evaluate_model(self, test_data: List[Dict], max_new_tokens: int = 200, 
                      temperature: float = 0.7, batch_size: int = 1) -> Dict:
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        logger.info("ğŸ§ª å¼€å§‹æ¨¡å‹è¯„ä¼°...")
        
        predictions = []
        targets = []
        generation_errors = 0
        
        with torch.no_grad():
            for i, sample in enumerate(tqdm(test_data, desc="è¯„ä¼°è¿›åº¦")):
                try:
                    # è§£ææ ·æœ¬
                    server_instruction, client_instructions, target_output = self.parse_qwen_sample(sample)
                    
                    # ç”Ÿæˆé¢„æµ‹
                    generated_text = self.model.generate(
                        server_instruction,
                        client_instructions,
                        max_new_tokens=max_new_tokens,
                        temperature=temperature,
                        do_sample=True
                    )
                    
                    predictions.append(generated_text)
                    targets.append(target_output)
                    
                except Exception as e:
                    logger.warning(f"æ ·æœ¬ {i} ç”Ÿæˆå¤±è´¥: {e}")
                    generation_errors += 1
                    predictions.append("ç”Ÿæˆå¤±è´¥")
                    targets.append(sample.get('output', ''))
        
        # è®¡ç®—æŒ‡æ ‡
        logger.info("ğŸ“Š è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
        metrics = self.calculate_metrics(predictions, targets)
        metrics['generation_errors'] = generation_errors
        metrics['generation_success_rate'] = 1.0 - (generation_errors / len(test_data))
        
        return {
            'metrics': metrics,
            'predictions': predictions,
            'targets': targets
        }
    
    def print_evaluation_report(self, results: Dict):
        """æ‰“å°è¯¦ç»†è¯„ä¼°æŠ¥å‘Š"""
        metrics = results['metrics']
        predictions = results['predictions']
        targets = results['targets']
        
        logger.info("=" * 80)
        logger.info("ğŸ“ˆ è”é‚¦Qwenæ¨¡å‹è¯¦ç»†è¯„ä¼°æŠ¥å‘Š")
        logger.info("=" * 80)
        
        logger.info(f"ğŸ“Š åŸºç¡€æŒ‡æ ‡:")
        logger.info(f"   - æ€»æ ·æœ¬æ•°: {metrics['total_samples']}")
        logger.info(f"   - ç”ŸæˆæˆåŠŸç‡: {metrics['generation_success_rate']:.2%}")
        logger.info(f"   - ç”Ÿæˆé”™è¯¯æ•°: {metrics['generation_errors']}")
        
        logger.info(f"ğŸ¯ ä»»åŠ¡å‡†ç¡®ç‡:")
        logger.info(f"   - è¿è¾“æ–¹å¼å‡†ç¡®ç‡: {metrics['transport_accuracy']:.2%}")
        logger.info(f"   - ç›®çš„åœ°åŸå¸‚å‡†ç¡®ç‡: {metrics['city_accuracy']:.2%}")
        logger.info(f"   - ç›®çš„åœ°çœä»½å‡†ç¡®ç‡: {metrics['province_accuracy']:.2%}")
        
        logger.info(f"ğŸ“ æ–‡æœ¬è´¨é‡æŒ‡æ ‡:")
        logger.info(f"   - å¹³å‡BLEUåˆ†æ•°: {metrics['avg_bleu_score']:.4f}")
        logger.info(f"   - å®Œå…¨åŒ¹é…å‡†ç¡®ç‡: {metrics['exact_match_accuracy']:.2%}")
        
        # è¯¦ç»†åˆ†ç±»æŒ‡æ ‡æŠ¥å‘Š
        self._print_task_metrics(metrics, 'transport', 'ğŸš› è¿è¾“æ–¹å¼åˆ†ç±»æŒ‡æ ‡')
        self._print_task_metrics(metrics, 'city', 'ğŸ™ï¸ ç›®çš„åœ°åŸå¸‚é¢„æµ‹æŒ‡æ ‡')
        self._print_task_metrics(metrics, 'province', 'ğŸ—ºï¸ ç›®çš„åœ°çœä»½é¢„æµ‹æŒ‡æ ‡')
        
        # æ··æ·†çŸ©é˜µå±•ç¤º
        self._print_confusion_matrices(metrics)
        
        # æ˜¾ç¤ºä¸€äº›ç¤ºä¾‹
        logger.info(f"ğŸ” ç”Ÿæˆç¤ºä¾‹ (å‰3ä¸ª):")
        for i in range(min(3, len(predictions))):
            logger.info(f"   æ ·æœ¬ {i+1}:")
            logger.info(f"     ç›®æ ‡: {targets[i][:100]}...")
            logger.info(f"     é¢„æµ‹: {predictions[i][:100]}...")
            logger.info(f"     ---")
        
        logger.info("=" * 80)
    
    def _print_task_metrics(self, metrics: Dict, task_name: str, title: str):
        """æ‰“å°å•ä¸ªä»»åŠ¡çš„è¯¦ç»†æŒ‡æ ‡"""
        logger.info(f"\n{title}:")
        
        # å®å¹³å‡æŒ‡æ ‡
        macro_precision = metrics.get(f'{task_name}_macro_precision', 0.0)
        macro_recall = metrics.get(f'{task_name}_macro_recall', 0.0)
        macro_f1 = metrics.get(f'{task_name}_macro_f1', 0.0)
        
        logger.info(f"   ğŸ“Š Macroå¹³å‡:")
        logger.info(f"      - ç²¾ç¡®ç‡ (Precision): {macro_precision:.4f}")
        logger.info(f"      - å¬å›ç‡ (Recall): {macro_recall:.4f}")
        logger.info(f"      - F1å€¼: {macro_f1:.4f}")
        
        # åŠ æƒå¹³å‡æŒ‡æ ‡
        weighted_precision = metrics.get(f'{task_name}_weighted_precision', 0.0)
        weighted_recall = metrics.get(f'{task_name}_weighted_recall', 0.0)
        weighted_f1 = metrics.get(f'{task_name}_weighted_f1', 0.0)
        
        logger.info(f"   âš–ï¸ Weightedå¹³å‡:")
        logger.info(f"      - ç²¾ç¡®ç‡ (Precision): {weighted_precision:.4f}")
        logger.info(f"      - å¬å›ç‡ (Recall): {weighted_recall:.4f}")
        logger.info(f"      - F1å€¼: {weighted_f1:.4f}")
        
        # æ¯ä¸ªç±»åˆ«çš„è¯¦ç»†æŒ‡æ ‡
        per_class_metrics = metrics.get(f'{task_name}_per_class', {})
        if per_class_metrics:
            logger.info(f"   ğŸ·ï¸ å„ç±»åˆ«æŒ‡æ ‡:")
            for label, class_metrics in per_class_metrics.items():
                precision = class_metrics['precision']
                recall = class_metrics['recall']
                f1 = class_metrics['f1']
                support = class_metrics['support']
                logger.info(f"      - {label}: P={precision:.3f}, R={recall:.3f}, F1={f1:.3f}, æ ·æœ¬æ•°={support}")
        
        # ç±»åˆ«åˆ†å¸ƒ
        label_distribution = metrics.get(f'{task_name}_label_distribution', {})
        if label_distribution:
            logger.info(f"   ğŸ“ˆ ç±»åˆ«åˆ†å¸ƒ:")
            total_samples = sum(label_distribution.values())
            for label, count in sorted(label_distribution.items(), key=lambda x: x[1], reverse=True):
                percentage = count / total_samples * 100
                logger.info(f"      - {label}: {count} ({percentage:.1f}%)")
    
    def _print_confusion_matrices(self, metrics: Dict):
        """æ‰“å°æ··æ·†çŸ©é˜µ"""
        confusion_matrices = metrics.get('confusion_matrices', {})
        
        for task_name, matrix in confusion_matrices.items():
            if not matrix:
                continue
                
            task_titles = {
                'transport': 'ğŸš› è¿è¾“æ–¹å¼æ··æ·†çŸ©é˜µ',
                'city': 'ğŸ™ï¸ åŸå¸‚é¢„æµ‹æ··æ·†çŸ©é˜µ', 
                'province': 'ğŸ—ºï¸ çœä»½é¢„æµ‹æ··æ·†çŸ©é˜µ'
            }
            
            logger.info(f"\n{task_titles.get(task_name, f'{task_name} æ··æ·†çŸ©é˜µ')}:")
            
            # è·å–æ‰€æœ‰æ ‡ç­¾
            all_labels = sorted(set(list(matrix.keys()) + [pred for pred_dict in matrix.values() for pred in pred_dict.keys()]))
            
            # åªæ˜¾ç¤ºå‰5ä¸ªæœ€å¸¸è§çš„æ ‡ç­¾ï¼Œé¿å…è¾“å‡ºè¿‡é•¿
            if len(all_labels) > 5:
                # æŒ‰é¢‘ç‡æ’åºï¼Œé€‰æ‹©å‰5ä¸ª
                label_counts = {}
                for true_label, pred_dict in matrix.items():
                    label_counts[true_label] = label_counts.get(true_label, 0) + sum(pred_dict.values())
                top_labels = sorted(label_counts.keys(), key=lambda x: label_counts[x], reverse=True)[:5]
                all_labels = top_labels
                logger.info(f"   (æ˜¾ç¤ºtop-{len(all_labels)}ç±»åˆ«)")
            
            # æ‰“å°çŸ©é˜µå¤´éƒ¨
            header = "çœŸå®\\é¢„æµ‹".ljust(12)
            for label in all_labels:
                header += f"{label[:8]:>8}"
            logger.info(f"   {header}")
            
            # æ‰“å°çŸ©é˜µå†…å®¹
            for true_label in all_labels:
                row = f"{true_label[:10]:10}"
                for pred_label in all_labels:
                    count = matrix.get(true_label, {}).get(pred_label, 0)
                    row += f"{count:>8}"
                logger.info(f"   {row}")
    
    def save_evaluation_results(self, results: Dict, output_file: str):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        logger.info(f"ğŸ’¾ ä¿å­˜è¯„ä¼°ç»“æœåˆ°: {output_file}")
        
        # å‡†å¤‡ä¿å­˜æ•°æ®
        save_data = {
            'metrics': results['metrics'],
            'model_path': self.model_path,
            'checkpoint_path': self.checkpoint_path,
            'evaluation_samples': []
        }
        
        # æ·»åŠ æ ·æœ¬è¯¦æƒ…ï¼ˆé™åˆ¶æ•°é‡ä»¥é¿å…æ–‡ä»¶è¿‡å¤§ï¼‰
        max_save_samples = min(100, len(results['predictions']))
        for i in range(max_save_samples):
            save_data['evaluation_samples'].append({
                'sample_id': i,
                'prediction': results['predictions'][i],
                'target': results['targets'][i]
            })
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2)
        
        logger.info("âœ… è¯„ä¼°ç»“æœä¿å­˜å®Œæˆ")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="è¯„ä¼°è”é‚¦Qwenæ¨¡å‹")
    parser.add_argument("--model_path", type=str,
                       default="/root/autodl-tmp/Federated_learning/llm_init_ckpt/Qwen2.5-0.5B-Instruct",
                       help="é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„")
    parser.add_argument("--checkpoint_path", type=str,
                       default="federated_qwen_output/federated_qwen_model.pth",
                       help="è®­ç»ƒåçš„æ£€æŸ¥ç‚¹è·¯å¾„")
    parser.add_argument("--test_data", type=str,
                       default="./data/qwen_processed/qwen_federated_train.jsonl",
                       help="æµ‹è¯•æ•°æ®æ–‡ä»¶")
    parser.add_argument("--max_samples", type=int, default=100,
                       help="æœ€å¤§æµ‹è¯•æ ·æœ¬æ•°")
    parser.add_argument("--max_new_tokens", type=int, default=200,
                       help="ç”Ÿæˆçš„æœ€å¤§tokenæ•°")
    parser.add_argument("--temperature", type=float, default=0.7,
                       help="ç”Ÿæˆæ¸©åº¦")
    parser.add_argument("--output_file", type=str, default="evaluation_results.json",
                       help="è¯„ä¼°ç»“æœè¾“å‡ºæ–‡ä»¶")
    parser.add_argument("--device", type=str, default="cuda",
                       help="è®¡ç®—è®¾å¤‡")
    
    args = parser.parse_args()
    
    try:
        # åˆå§‹åŒ–è¯„ä¼°å™¨
        evaluator = FederatedModelEvaluator(
            model_path=args.model_path,
            checkpoint_path=args.checkpoint_path,
            device=args.device
        )
        
        # åŠ è½½æµ‹è¯•æ•°æ®
        test_data_file = Path(args.test_data)
        if not test_data_file.exists():
            logger.error(f"æµ‹è¯•æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {test_data_file}")
            return False
        
        test_data = evaluator.load_test_data(test_data_file, args.max_samples)
        
        if len(test_data) == 0:
            logger.error("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æµ‹è¯•æ•°æ®")
            return False
        
        # æ‰§è¡Œè¯„ä¼°
        results = evaluator.evaluate_model(
            test_data=test_data,
            max_new_tokens=args.max_new_tokens,
            temperature=args.temperature
        )
        
        # æ‰“å°æŠ¥å‘Š
        evaluator.print_evaluation_report(results)
        
        # ä¿å­˜ç»“æœ
        evaluator.save_evaluation_results(results, args.output_file)
        
        logger.info("ğŸ‰ æ¨¡å‹è¯„ä¼°å®Œæˆï¼")
        return True
        
    except Exception as e:
        logger.error(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    if not success:
        exit(1)