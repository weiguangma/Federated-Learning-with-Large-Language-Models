#!/usr/bin/env python3

import pandas as pd
import numpy as np
import json
import logging
import argparse
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from tqdm import tqdm
import time

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class TextConverter:
    """æ–‡æœ¬è½¬æ¢å™¨åŸºç±»"""
    def __init__(self):
        self.missing_value_handlers = {
            'numeric': lambda x: f"çº¦{np.random.randint(1, 100)}å•ä½" if pd.isna(x) else str(x),
            'categorical': lambda x: "æœªçŸ¥ç±»å‹" if pd.isna(x) else str(x),
            'time': lambda x: "æ—¶é—´æœªçŸ¥" if pd.isna(x) else str(x)
        }
    
    def safe_convert(self, value, value_type='categorical'):
        """å®‰å…¨è½¬æ¢å€¼"""
        try:
            if pd.isna(value) or value is None or str(value).strip() == '':
                return self.missing_value_handlers[value_type](value)
            return str(value).strip()
        except:
            return "æ•°æ®å¼‚å¸¸"

class PortTextConverter(TextConverter):
    """æ¸¯å£æ•°æ®æ–‡æœ¬è½¬æ¢å™¨"""
    def convert_to_text(self, row: Dict) -> str:
        try:
            # æå–å…³é”®ä¿¡æ¯
            cargo_type = self.safe_convert(row.get('è´§ç‰©ç±»å‹', ''), 'categorical')
            weight = self.safe_convert(row.get('é‡é‡', ''), 'numeric')
            container_type = self.safe_convert(row.get('è£…ç®±æ–¹å¼', ''), 'categorical')
            trade_direction = self.safe_convert(row.get('è´¸æ˜“æ–¹å‘', ''), 'categorical')
            dest_port = self.safe_convert(row.get('ç›®çš„æ¸¯', ''), 'categorical')
            berth_time = self.safe_convert(row.get('ç­‰å¾…æ—¶é—´', ''), 'numeric')
            yard_time = self.safe_convert(row.get('åœç•™æ—¶é—´', ''), 'numeric')
            
            text = f"""<|æ¸¯å£ä¸“å®¶|>
ä½œä¸ºæ¸¯å£è¿è¥ä¸“å®¶ï¼Œæˆ‘æä¾›ä»¥ä¸‹æ¸¯å£ä½œä¸šå’Œè´§ç‰©ä¿¡æ¯ï¼š
è´§ç‰©ç±»å‹ï¼š{cargo_type}ã€‚è£…ç®±æ–¹å¼ï¼š{container_type}ã€‚è´¸æ˜“æ–¹å‘ï¼š{trade_direction}ã€‚ç›®çš„æ¸¯ï¼š{dest_port}æ¸¯ã€‚æ¸¯å£ä½œä¸šï¼šé æ³Šæ­£å¸¸ï¼Œç­‰å¾…æ—¶é—´{berth_time}å°æ—¶ã€‚å †åœºæƒ…å†µï¼šå †åœºå‘¨è½¬æ­£å¸¸ï¼Œåœç•™{yard_time}å°æ—¶ã€‚"""
            
            return text.strip()
        except Exception as e:
            logger.warning(f"æ¸¯å£æ•°æ®è½¬æ¢å¤±è´¥: {e}")
            return "<|æ¸¯å£ä¸“å®¶|>\nä½œä¸ºæ¸¯å£è¿è¥ä¸“å®¶ï¼Œå½“å‰æ¸¯å£æ•°æ®æš‚æ—¶æ— æ³•è·å–ã€‚"

class RailwayTextConverter(TextConverter):
    """é“è·¯æ•°æ®æ–‡æœ¬è½¬æ¢å™¨"""
    def convert_to_text(self, row: Dict) -> str:
        try:
            # æå–å…³é”®ä¿¡æ¯
            scale = self.safe_convert(row.get('è¿è¾“è§„æ¨¡', ''), 'categorical')
            container_count = self.safe_convert(row.get('é›†è£…ç®±æ•°é‡', ''), 'numeric')
            total_weight = self.safe_convert(row.get('æ€»é‡é‡', ''), 'numeric')
            cost_level = self.safe_convert(row.get('è´¹ç”¨æ°´å¹³', ''), 'categorical')
            total_cost = self.safe_convert(row.get('æ€»è´¹ç”¨', ''), 'numeric')
            rail_price = self.safe_convert(row.get('é“è·¯æŠ¥ä»·', ''), 'numeric')
            distance = self.safe_convert(row.get('è¿è¾“è·ç¦»', ''), 'numeric')
            time_efficiency = self.safe_convert(row.get('è¿è¾“æ—¶æ•ˆ', ''), 'categorical')
            estimated_time = self.safe_convert(row.get('é¢„è®¡æ—¶é—´', ''), 'numeric')
            route = self.safe_convert(row.get('è¿è¾“è·¯çº¿', ''), 'categorical')
            
            text = f"""<|é“è·¯ä¸“å®¶|>  
ä½œä¸ºé“è·¯è¿è¾“ä¸“å®¶ï¼Œæˆ‘æä¾›ä»¥ä¸‹é“è·¯è¿è¾“å’Œè·¯çº¿ä¿¡æ¯ï¼š
è¿è¾“è§„æ¨¡ï¼š{scale}ï¼Œ{container_count}ä¸ªé›†è£…ç®±ï¼Œæ€»é‡{total_weight}å¨ã€‚è´¹ç”¨æ°´å¹³ï¼š{cost_level}ï¼Œæ€»è´¹ç”¨{total_cost}å…ƒã€‚é“è·¯æŠ¥ä»·ï¼š{rail_price}å…ƒã€‚è¿è¾“è·ç¦»ï¼šä¸­è·ç¦»è¿è¾“ï¼Œ{distance}å…¬é‡Œã€‚è¿è¾“æ—¶æ•ˆï¼š{time_efficiency}ï¼Œé¢„è®¡{estimated_time}å°æ—¶ã€‚è¿è¾“è·¯çº¿ï¼š{route}ã€‚"""
            
            return text.strip()
        except Exception as e:
            logger.warning(f"é“è·¯æ•°æ®è½¬æ¢å¤±è´¥: {e}")
            return "<|é“è·¯ä¸“å®¶|>\nä½œä¸ºé“è·¯è¿è¾“ä¸“å®¶ï¼Œå½“å‰é“è·¯æ•°æ®æš‚æ—¶æ— æ³•è·å–ã€‚"

class CustomsTextConverter(TextConverter):
    """æµ·å…³æ•°æ®æ–‡æœ¬è½¬æ¢å™¨"""
    def convert_to_text(self, row: Dict) -> str:
        try:
            # æå–å…³é”®ä¿¡æ¯
            product_type = self.safe_convert(row.get('å•†å“ç‰¹å¾', ''), 'categorical')
            weight = self.safe_convert(row.get('é‡é‡', ''), 'numeric')
            
            text = f"""<|æµ·å…³ä¸“å®¶|>
ä½œä¸ºæµ·å…³ä¸šåŠ¡ä¸“å®¶ï¼Œæˆ‘æä¾›ä»¥ä¸‹è´¸æ˜“å’Œæ¸…å…³ä¿¡æ¯ï¼š
å•†å“ç‰¹å¾ï¼š{product_type}ï¼Œé‡é‡{weight}å…¬æ–¤ã€‚"""
            
            return text.strip()
        except Exception as e:
            logger.warning(f"æµ·å…³æ•°æ®è½¬æ¢å¤±è´¥: {e}")
            return "<|æµ·å…³ä¸“å®¶|>\nä½œä¸ºæµ·å…³ä¸šåŠ¡ä¸“å®¶ï¼Œå½“å‰æµ·å…³æ•°æ®æš‚æ—¶æ— æ³•è·å–ã€‚"

class FederatedDataIntegrator:
    """è”é‚¦æ•°æ®é›†æˆå™¨ - é«˜æ€§èƒ½ç‰ˆæœ¬"""
    
    def __init__(self, data_dir: str):
        self.data_dir = Path(data_dir)
        self.port_converter = PortTextConverter()
        self.railway_converter = RailwayTextConverter()
        self.customs_converter = CustomsTextConverter()
        
        # æ•°æ®æ–‡ä»¶è·¯å¾„
        self.data_files = {
            'port': self.data_dir / 'æ¸¯å£æ•°æ®_å¤„ç†å.csv',
            'railway': self.data_dir / 'é“è·¯åŸå§‹æ•°æ®_è¡¥å……ä¸æ¨¡æ‹Ÿ_å«å…¬è·¯ç‰¹å¾.csv',
            'customs_railway': self.data_dir / 'é“è·¯æµ·å…³æ¨¡æ‹Ÿæ•°æ®.csv',
            'customs_potential': self.data_dir / 'æ½œåœ¨ç®±æº_æµ·å…³æ¨¡æ‹Ÿ_æ ·ä¾‹å…¨.csv',
            'potential': self.data_dir / 'æ½œåœ¨ç®±æºæ¨¡æ‹Ÿæ•°æ®.csv'
        }
        
        logger.info(f"ğŸ“ æ•°æ®ç›®å½•: {self.data_dir}")
        
    def load_data(self) -> Dict[str, pd.DataFrame]:
        """åŠ è½½æ‰€æœ‰æ•°æ®æ–‡ä»¶"""
        data = {}
        
        for name, file_path in self.data_files.items():
            if file_path.exists():
                try:
                    df = pd.read_csv(file_path, encoding='utf-8')
                    data[name] = df
                    logger.info(f"âœ… åŠ è½½ {name}: {len(df)} è¡Œ")
                except Exception as e:
                    logger.warning(f"âš ï¸  åŠ è½½ {name} å¤±è´¥: {e}")
                    data[name] = pd.DataFrame()
            else:
                logger.warning(f"âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                data[name] = pd.DataFrame()
        
        return data
    
    def create_federated_samples(self, data: Dict[str, pd.DataFrame], sample_size: int = 2000) -> List[Dict]:
        """åˆ›å»ºè”é‚¦æ ·æœ¬ - ä¼˜åŒ–ç‰ˆæœ¬"""
        logger.info(f"ğŸ”§ å¼€å§‹åˆ›å»º {sample_size} ä¸ªè”é‚¦æ ·æœ¬...")
        
        # ä½¿ç”¨æ¸¯å£æ•°æ®ä½œä¸ºä¸»æ•°æ®æº
        port_data = data['port']
        railway_data = data['railway']
        customs_railway_data = data['customs_railway']
        customs_potential_data = data['customs_potential']
        
        if port_data.empty:
            logger.error("âŒ æ¸¯å£æ•°æ®ä¸ºç©ºï¼Œæ— æ³•åˆ›å»ºæ ·æœ¬")
            return []
        
        # åˆ›å»ºé«˜æ•ˆç´¢å¼• - O(1)æŸ¥æ‰¾
        logger.info("ğŸ” åˆ›å»ºé«˜æ•ˆç´¢å¼•...")
        
        # ç¡®ä¿CNTRåˆ—ä¸ºå­—ç¬¦ä¸²ç±»å‹
        if 'CNTR' in railway_data.columns:
            railway_data['CNTR'] = railway_data['CNTR'].astype(str)
        if 'XH' in railway_data.columns:
            railway_data['XH'] = railway_data['XH'].astype(str)
        if 'CNTR' in customs_railway_data.columns:
            customs_railway_data['CNTR'] = customs_railway_data['CNTR'].astype(str)
        if 'CNTR' in port_data.columns:
            port_data['CNTR'] = port_data['CNTR'].astype(str)
            
        # åˆ›å»ºå“ˆå¸Œç´¢å¼• - å¤„ç†é‡å¤ç´¢å¼•
        railway_cntr_index = {}
        if 'CNTR' in railway_data.columns:
            # å»é‡ååˆ›å»ºç´¢å¼•
            railway_data_dedup = railway_data.drop_duplicates(subset=['CNTR'], keep='first')
            railway_cntr_index = railway_data_dedup.set_index('CNTR').to_dict('index')
        
        railway_xh_index = {}
        if 'XH' in railway_data.columns:
            # å»é‡ååˆ›å»ºç´¢å¼•
            railway_data_dedup_xh = railway_data.drop_duplicates(subset=['XH'], keep='first')
            railway_xh_index = railway_data_dedup_xh.set_index('XH').to_dict('index')
        
        customs_cntr_index = {}
        if 'CNTR' in customs_railway_data.columns:
            # å»é‡ååˆ›å»ºç´¢å¼•
            customs_data_dedup = customs_railway_data.drop_duplicates(subset=['CNTR'], keep='first')
            customs_cntr_index = customs_data_dedup.set_index('CNTR').to_dict('index')
        
        logger.info(f"ğŸ“Š ç´¢å¼•ç»Ÿè®¡: é“è·¯CNTR={len(railway_cntr_index)}, é“è·¯XH={len(railway_xh_index)}, æµ·å…³CNTR={len(customs_cntr_index)}")
        
        federated_samples = []
        start_time = time.time()
        
        # é™åˆ¶æ ·æœ¬æ•°é‡
        total_samples = min(sample_size, len(port_data))
        
        for idx in tqdm(range(total_samples), desc="åˆ›å»ºè”é‚¦æ ·æœ¬"):
            try:
                port_row = port_data.iloc[idx]
                
                # å®‰å…¨æå–container_id
                try:
                    cntr_value = port_row.get('CNTR', f'sample_{idx}')
                    if hasattr(cntr_value, 'iloc'):
                        cntr_value = cntr_value.iloc[0] if len(cntr_value) > 0 else f'sample_{idx}'
                    if cntr_value is None or pd.isna(cntr_value) or str(cntr_value).strip() == '':
                        container_id = f'sample_{idx}'
                    else:
                        container_id = str(cntr_value).strip()
                except Exception:
                    container_id = f'sample_{idx}'
                
                # O(1)æŸ¥æ‰¾åŒ¹é…çš„é“è·¯æ•°æ®
                railway_row = railway_cntr_index.get(container_id)
                if railway_row is None:
                    railway_row = railway_xh_index.get(container_id)
                
                # O(1)æŸ¥æ‰¾åŒ¹é…çš„æµ·å…³æ•°æ®
                customs_row = customs_cntr_index.get(container_id)
                
                # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                try:
                    if port_row is not None and hasattr(port_row, 'to_dict'):
                        port_dict = port_row.to_dict()
                    elif port_row is not None and hasattr(port_row, 'iloc'):
                        port_dict = dict(port_row)
                    else:
                        port_dict = None
                except Exception:
                    port_dict = None
                
                # ç”Ÿæˆæ–‡æœ¬æè¿°
                port_text = self.port_converter.convert_to_text(port_dict) if port_dict else "<|æ¸¯å£ä¸“å®¶|>\næ¸¯å£æ•°æ®æš‚æ—¶æ— æ³•è·å–ã€‚"
                railway_text = self.railway_converter.convert_to_text(railway_row) if railway_row else "<|é“è·¯ä¸“å®¶|>\né“è·¯æ•°æ®æš‚æ—¶æ— æ³•è·å–ã€‚"
                customs_text = self.customs_converter.convert_to_text(customs_row) if customs_row else "<|æµ·å…³ä¸“å®¶|>\næµ·å…³æ•°æ®æš‚æ—¶æ— æ³•è·å–ã€‚"
                
                # ç”Ÿæˆå†³ç­–è¾“å‡º
                output_text = self._generate_output(port_dict, railway_row, customs_row)
                
                # åˆ›å»ºè”é‚¦æ ·æœ¬
                sample = {
                    'container_id': container_id,
                    'port_text': port_text,
                    'railway_text': railway_text,
                    'customs_text': customs_text,
                    'output': output_text,
                    'sample_id': f'federated_{idx:08d}'
                }
                
                federated_samples.append(sample)
                
            except Exception as e:
                logger.warning(f"åˆ›å»ºè”é‚¦æ ·æœ¬å¤±è´¥ [{idx}]: {e}")
                continue
        
        end_time = time.time()
        processing_time = end_time - start_time
        
        if len(federated_samples) > 0:
            avg_time = processing_time / len(federated_samples)
            logger.info(f"âœ… è”é‚¦æ ·æœ¬åˆ›å»ºå®Œæˆ: {len(federated_samples)} ä¸ª")
            logger.info(f"â±ï¸  å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’, å¹³å‡æ¯æ ·æœ¬: {avg_time:.4f}ç§’")
        else:
            logger.error("âŒ æ²¡æœ‰æˆåŠŸåˆ›å»ºä»»ä½•è”é‚¦æ ·æœ¬")
        
        return federated_samples
    
    def _generate_output(self, port_data: Dict, railway_data: Dict, customs_data: Dict) -> str:
        """ç”Ÿæˆå†³ç­–è¾“å‡º"""
        try:
            # ç®€å•çš„å†³ç­–é€»è¾‘
            transport_mode = "é“è·¯è¿è¾“"
            destination = "æœªçŸ¥"
            province = "æœªçŸ¥"
            
            # ä»é“è·¯æ•°æ®æå–ç›®çš„åœ°
            if railway_data and 'è¿è¾“è·¯çº¿' in railway_data:
                route = str(railway_data['è¿è¾“è·¯çº¿'])
                if 'åˆ°' in route:
                    destination = route.split('åˆ°')[-1].replace('ç«™', '').strip()
            
            output = f"""åŸºäºå¤šæºä¿¡æ¯ç»¼åˆåˆ†æï¼š

**è¿è¾“æ–¹å¼é€‰æ‹©**: {transport_mode}
**ç›®çš„åœ°åŸå¸‚**: {destination}  
**ç›®çš„åœ°çœä»½**: {province}

**å†³ç­–ç†ç”±**:
1. **å¤šæºæ•°æ®èåˆ**: ç»¼åˆæ¸¯å£ä½œä¸šæ•ˆç‡ã€é“è·¯è¿è¾“æˆæœ¬ã€æµ·å…³æ¸…å…³æ—¶æ•ˆç­‰å…³é”®å› ç´ 
2. **è¿è¾“æ–¹å¼ä¼˜é€‰**: é€‰æ‹©{transport_mode}åŸºäºæˆæœ¬æ•ˆç›Šå’Œæ—¶æ•ˆæ€§å¹³è¡¡
3. **ç›®çš„åœ°é¢„æµ‹**: æ ¹æ®è´§ç‰©ç‰¹å¾å’Œè´¸æ˜“æµå‘ï¼Œç¡®å®šç›®çš„åœ°ä¸º{province}{destination}
4. **é£é™©è¯„ä¼°**: è€ƒè™‘è¿è¾“è·¯å¾„ã€å¤©æ°”æ¡ä»¶ã€æ”¿ç­–å½±å“ç­‰é£é™©å› ç´ 
5. **ä¼˜åŒ–å»ºè®®**: å»ºè®®é‡‡ç”¨å¤šå¼è”è¿ä»¥æé«˜æ•´ä½“æ•ˆç‡"""
            
            return output
        except Exception as e:
            logger.warning(f"ç”Ÿæˆè¾“å‡ºå¤±è´¥: {e}")
            return "åŸºäºå½“å‰å¯ç”¨ä¿¡æ¯ï¼Œå»ºè®®é‡‡ç”¨ç»¼åˆè¿è¾“æ–¹æ¡ˆã€‚"

class QwenFederatedFormatter:
    """Qwenè”é‚¦æ ¼å¼åŒ–å™¨"""
    
    def __init__(self):
        self.separator = " <|object_ref_start|> "
        self.end_separator = " <|object_ref_end|> "
    
    def is_valid_sample(self, sample: Dict) -> bool:
        """æ£€æŸ¥æ ·æœ¬æ˜¯å¦æœ‰æ•ˆ"""
        try:
            # æ£€æŸ¥å¿…è¦å­—æ®µæ˜¯å¦å­˜åœ¨
            required_fields = ['port_text', 'railway_text', 'customs_text', 'output']
            for field in required_fields:
                if field not in sample or not sample[field]:
                    return False
            
            # æ£€æŸ¥å®¢æˆ·ç«¯ä¿¡æ¯æ˜¯å¦æœ‰æ•ˆï¼ˆä¸èƒ½æ˜¯"æ•°æ®æš‚æ—¶æ— æ³•è·å–"ï¼‰
            invalid_phrases = [
                "æ•°æ®æš‚æ—¶æ— æ³•è·å–",
                "æš‚æ—¶æ— æ³•è·å–",
                "æ— æ³•è·å–",
                "æ•°æ®ç¼ºå¤±",
                "ä¿¡æ¯ä¸å®Œæ•´"
            ]
            
            for field in ['port_text', 'railway_text', 'customs_text']:
                text = sample[field].lower()
                if any(phrase in text for phrase in invalid_phrases):
                    return False
            
            # æ£€æŸ¥æ¸¯å£ä¿¡æ¯æ˜¯å¦å®Œæ•´
            port_text = sample['port_text']
            required_port_info = ['è´§ç‰©ç±»å‹ï¼š', 'è£…ç®±æ–¹å¼ï¼š', 'è´¸æ˜“æ–¹å‘ï¼š', 'ç›®çš„æ¸¯ï¼š']
            for info in required_port_info:
                if info in port_text:
                    # æ£€æŸ¥å†’å·åæ˜¯å¦æœ‰å®é™…å†…å®¹ï¼ˆä¸èƒ½åªæ˜¯ç©ºæ ¼æˆ–"ã€‚"ï¼‰
                    after_colon = port_text.split(info)[1].split('ã€‚')[0].strip()
                    if not after_colon or after_colon in ['', 'æ— ', 'æœªçŸ¥', '-']:
                        return False
            
            # æ£€æŸ¥è¾“å‡ºä¸­çš„ç›®çš„åœ°ä¿¡æ¯
            output_text = sample['output']
            if 'ç›®çš„åœ°åŸå¸‚**: æœªçŸ¥' in output_text or 'ç›®çš„åœ°çœä»½**: æœªçŸ¥' in output_text:
                return False
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«æœ‰æ•ˆçš„è¿è¾“æ–¹å¼
            if 'è¿è¾“æ–¹å¼é€‰æ‹©**:' not in output_text:
                return False
            
            return True
            
        except Exception as e:
            logger.warning(f"æ ·æœ¬éªŒè¯å¤±è´¥: {e}")
            return False
    
    def format_to_qwen(self, sample: Dict) -> Dict[str, str]:
        """è½¬æ¢ä¸ºQwenè®­ç»ƒæ ¼å¼"""
        try:
            # é¦–å…ˆæ£€æŸ¥æ ·æœ¬æ˜¯å¦æœ‰æ•ˆ
            if not self.is_valid_sample(sample):
                return None
            
            # æ„å»ºinstruction
            server_instruction = "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ç‰©æµå†³ç­–ç³»ç»Ÿã€‚åŸºäºå¤šä¸ªå®¢æˆ·ç«¯æä¾›çš„ä¸“ä¸šä¿¡æ¯ï¼Œè¯·ç»¼åˆåˆ†æå¹¶åšå‡ºæœ€ä¼˜çš„è¿è¾“å†³ç­–ã€‚\n\nè¯·æ ¹æ®ä»¥ä¸‹å¤šæºæ•°æ®ä¿¡æ¯ï¼Œé€‰æ‹©æœ€é€‚åˆçš„è¿è¾“æ–¹å¼ï¼Œé¢„æµ‹ç›®çš„åœ°ï¼Œå¹¶æä¾›è¯¦ç»†çš„å†³ç­–ç†ç”±ï¼š"
            
            instruction = (
                server_instruction +
                self.separator + sample['port_text'] + self.end_separator +
                self.separator + sample['railway_text'] + self.end_separator +
                self.separator + sample['customs_text'] + self.end_separator
            )
            
            return {
                "instruction": instruction,
                "input": "",
                "output": sample['output'],
                "sample_id": sample['sample_id']
            }
        except Exception as e:
            logger.error(f"æ ¼å¼åŒ–æ ·æœ¬å¤±è´¥: {e}")
            return None

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="è”é‚¦å­¦ä¹ æ•°æ®å¤„ç†")
    parser.add_argument("--data_dir", type=str, 
                       default="/root/autodl-tmp/Federated_learning/code_v01/verify_data",
                       help="åŸå§‹æ•°æ®ç›®å½•")
    parser.add_argument("--output_dir", type=str, default="./data/qwen_processed",
                       help="è¾“å‡ºç›®å½•")
    parser.add_argument("--sample_size", type=int, default=2000,
                       help="ç”Ÿæˆæ ·æœ¬æ•°é‡")
    parser.add_argument("--train_ratio", type=float, default=0.8,
                       help="è®­ç»ƒé›†æ¯”ä¾‹ (é»˜è®¤: 0.8)")
    parser.add_argument("--random_seed", type=int, default=42,
                       help="éšæœºç§å­ (é»˜è®¤: 42)")
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    try:
        # 1. æ•°æ®é›†æˆ
        logger.info("ğŸš€ å¼€å§‹è”é‚¦æ•°æ®å¤„ç†...")
        integrator = FederatedDataIntegrator(args.data_dir)
        data = integrator.load_data()
        
        # 2. åˆ›å»ºè”é‚¦æ ·æœ¬ï¼ˆç”Ÿæˆæ›´å¤šæ ·æœ¬ä»¥ç¡®ä¿è¿‡æ»¤åæœ‰è¶³å¤Ÿçš„æœ‰æ•ˆæ ·æœ¬ï¼‰
        # é¢„ä¼°è¿‡æ»¤ç‡ï¼Œç”Ÿæˆæ›´å¤šæ ·æœ¬
        estimated_filter_rate = 0.3  # é¢„ä¼°30%çš„æ ·æœ¬ä¼šè¢«è¿‡æ»¤
        target_samples = int(args.sample_size / (1 - estimated_filter_rate))
        
        logger.info(f"ğŸ¯ ç›®æ ‡æœ‰æ•ˆæ ·æœ¬æ•°: {args.sample_size}")
        logger.info(f"ğŸ“Š è€ƒè™‘è¿‡æ»¤ç‡ï¼Œç”Ÿæˆæ ·æœ¬æ•°: {target_samples}")
        
        federated_samples = integrator.create_federated_samples(data, target_samples)
        
        if not federated_samples:
            logger.error("âŒ æ²¡æœ‰ç”Ÿæˆä»»ä½•æ ·æœ¬")
            return False
        
        # 3. æ ¼å¼åŒ–ä¸ºQwenæ ¼å¼å¹¶è¿‡æ»¤æ— æ•ˆæ ·æœ¬
        logger.info("ğŸ“ è½¬æ¢ä¸ºQwenè®­ç»ƒæ ¼å¼å¹¶è¿‡æ»¤æ— æ•ˆæ ·æœ¬...")
        formatter = QwenFederatedFormatter()
        qwen_samples = []
        filtered_count = 0
        
        for sample in tqdm(federated_samples, desc="æ ¼å¼åŒ–å’Œè¿‡æ»¤æ ·æœ¬"):
            qwen_sample = formatter.format_to_qwen(sample)
            if qwen_sample:
                qwen_samples.append(qwen_sample)
            else:
                filtered_count += 1
        
        logger.info(f"ğŸ“Š æ ·æœ¬è¿‡æ»¤ç»“æœ:")
        logger.info(f"   - åŸå§‹æ ·æœ¬æ•°: {len(federated_samples)}")
        logger.info(f"   - è¿‡æ»¤æ‰çš„æ ·æœ¬: {filtered_count}")
        logger.info(f"   - æœ‰æ•ˆæ ·æœ¬æ•°: {len(qwen_samples)}")
        logger.info(f"   - è¿‡æ»¤ç‡: {filtered_count/len(federated_samples):.1%}")
        
        # å¦‚æœæœ‰æ•ˆæ ·æœ¬æ•°é‡ä¸è¶³ï¼Œç”Ÿæˆæ›´å¤š
        if len(qwen_samples) < args.sample_size:
            logger.warning(f"âš ï¸ æœ‰æ•ˆæ ·æœ¬æ•°é‡ä¸è¶³ ({len(qwen_samples)} < {args.sample_size})")
            logger.info("ğŸ”„ ç”Ÿæˆæ›´å¤šæ ·æœ¬ä»¥è¾¾åˆ°ç›®æ ‡æ•°é‡...")
            
            additional_needed = args.sample_size - len(qwen_samples)
            additional_raw = int(additional_needed / (1 - filtered_count/len(federated_samples))) + 100
            
            additional_samples = integrator.create_federated_samples(data, additional_raw)
            for sample in tqdm(additional_samples, desc="ç”Ÿæˆé¢å¤–æ ·æœ¬"):
                if len(qwen_samples) >= args.sample_size:
                    break
                qwen_sample = formatter.format_to_qwen(sample)
                if qwen_sample:
                    qwen_samples.append(qwen_sample)
        
        # å¦‚æœæ ·æœ¬è¿‡å¤šï¼Œéšæœºé€‰æ‹©ç›®æ ‡æ•°é‡
        if len(qwen_samples) > args.sample_size:
            import random
            random.seed(args.random_seed)
            qwen_samples = random.sample(qwen_samples, args.sample_size)
            
        logger.info(f"âœ… æœ€ç»ˆæœ‰æ•ˆæ ·æœ¬æ•°: {len(qwen_samples)}")
        
        # 4. åˆ†å‰²è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        logger.info(f"ğŸ”€ åˆ†å‰²è®­ç»ƒé›†å’Œæµ‹è¯•é›†...")
        
        import random
        random.seed(args.random_seed)
        
        # éšæœºæ‰“ä¹±æ ·æœ¬
        shuffled_samples = qwen_samples.copy()
        random.shuffle(shuffled_samples)
        
        # è®¡ç®—åˆ†å‰²ç‚¹
        total_samples = len(shuffled_samples)
        train_size = int(total_samples * args.train_ratio)
        test_size = total_samples - train_size
        
        train_samples = shuffled_samples[:train_size]
        test_samples = shuffled_samples[train_size:]
        
        logger.info(f"ğŸ“Š æ•°æ®åˆ†å‰²ç»“æœ:")
        logger.info(f"   - æ€»æ ·æœ¬æ•°: {total_samples}")
        logger.info(f"   - è®­ç»ƒé›†: {len(train_samples)} ({args.train_ratio:.1%})")
        logger.info(f"   - æµ‹è¯•é›†: {len(test_samples)} ({1-args.train_ratio:.1%})")
        
        # 5. ä¿å­˜è®­ç»ƒé›†
        train_file = output_dir / "qwen_federated_train.jsonl"
        logger.info(f"ğŸ’¾ ä¿å­˜è®­ç»ƒé›†åˆ°: {train_file}")
        
        with open(train_file, 'w', encoding='utf-8') as f:
            for sample in train_samples:
                f.write(json.dumps(sample, ensure_ascii=False) + '\n')
        
        # 6. ä¿å­˜æµ‹è¯•é›†
        test_file = output_dir / "qwen_federated_test.jsonl"
        logger.info(f"ğŸ’¾ ä¿å­˜æµ‹è¯•é›†åˆ°: {test_file}")
        
        with open(test_file, 'w', encoding='utf-8') as f:
            for sample in test_samples:
                f.write(json.dumps(sample, ensure_ascii=False) + '\n')
        
        # 7. ä¿å­˜æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
        dataset_stats = {
            "total_samples": total_samples,
            "train_samples": len(train_samples),
            "test_samples": len(test_samples),
            "train_ratio": args.train_ratio,
            "random_seed": args.random_seed,
            "train_file": str(train_file),
            "test_file": str(test_file),
            "created_at": time.strftime("%Y-%m-%d %H:%M:%S")
        }
        
        stats_file = output_dir / "dataset_split_stats.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(dataset_stats, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ‰ æ•°æ®å¤„ç†å®Œæˆï¼")
        logger.info(f"   - åŸå§‹æ ·æœ¬: {len(federated_samples)}")
        logger.info(f"   - Qwenæ ¼å¼æ ·æœ¬: {len(qwen_samples)}")
        logger.info(f"   - è®­ç»ƒé›†æ–‡ä»¶: {train_file} ({train_file.stat().st_size / 1024 / 1024:.2f} MB)")
        logger.info(f"   - æµ‹è¯•é›†æ–‡ä»¶: {test_file} ({test_file.stat().st_size / 1024 / 1024:.2f} MB)")
        logger.info(f"   - ç»Ÿè®¡ä¿¡æ¯: {stats_file}")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ æ•°æ®å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    if not success:
        exit(1)
