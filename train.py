#!/usr/bin/env python3
# /********************************************************************************
#  * @Author: zhangqiuhong
#  * @Date: 2025-09-20
#  * @Description: è”é‚¦å­¦ä¹ Qwenæ¨¡å‹è®­ç»ƒè„šæœ¬ - æ¸…ç†ç‰ˆæœ¬
#  ********************************************************************************/

import torch
import torch.nn as nn
import json
import logging
import argparse
from pathlib import Path
from tqdm import tqdm
import random
import re
from typing import List, Dict, Tuple, Optional
from federated_model import FederatedQwenSystem

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def load_training_data(data_file: Path, max_samples: int = None):
    """åŠ è½½è®­ç»ƒæ•°æ®"""
    logger.info(f"ğŸ“Š ä» {data_file} åŠ è½½è®­ç»ƒæ•°æ®...")
    
    samples = []
    with open(data_file, 'r', encoding='utf-8') as f:
        for line_idx, line in enumerate(f):
            if max_samples and len(samples) >= max_samples:
                        break
                
            try:
                sample = json.loads(line.strip())
                samples.append(sample)
            except json.JSONDecodeError as e:
                logger.warning(f"è·³è¿‡æ— æ•ˆJSONè¡Œ {line_idx}: {e}")
                continue
    
    logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(samples)} ä¸ªè®­ç»ƒæ ·æœ¬")
    return samples

def parse_qwen_sample(sample: dict):
    """è§£æQwenæ ¼å¼çš„æ•°æ® - æ”¯æŒå¢å¼ºç‰ˆå’Œæ—§ç‰ˆæ ¼å¼"""
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯å¢å¼ºç‰ˆæ ¼å¼ï¼ˆç›´æ¥åŒ…å« port_expert, railway_expert, customs_expertï¼‰
    if 'port_expert' in sample and 'railway_expert' in sample and 'customs_expert' in sample:
        # å¢å¼ºç‰ˆæ ¼å¼
        server_instruction = sample.get('server_instruction', "è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯åšå‡ºæœ€ä¼˜è¿è¾“å†³ç­–")
        client_instructions = {
            'port': sample['port_expert'],
            'railway': sample['railway_expert'], 
            'customs': sample['customs_expert']
        }
        return server_instruction, client_instructions, sample['output']
    
    # æ—§ç‰ˆæ ¼å¼å¤„ç†
    full_instruction = sample['instruction']
    
    # ä»instructionä¸­æå–æœåŠ¡ç«¯å’Œå®¢æˆ·ç«¯æŒ‡ä»¤
    parts = full_instruction.split('<|object_ref_start|>')
    server_instruction = parts[0].strip()
    
    # æå–å®¢æˆ·ç«¯æŒ‡ä»¤
    client_instructions = {}
    client_names = ['port', 'railway', 'customs']  # ä½¿ç”¨æ­£ç¡®çš„å®¢æˆ·ç«¯åç§°
    
    for i in range(1, min(4, len(parts))):  # æœ€å¤š3ä¸ªå®¢æˆ·ç«¯
        client_part = parts[i]
        if '<|object_ref_end|>' in client_part:
            client_content = client_part.split('<|object_ref_end|>')[0].strip()
            client_name = client_names[i-1] if i-1 < len(client_names) else f'client_{i}'
            client_instructions[client_name] = client_content
    
    # ç¡®ä¿æœ‰3ä¸ªå®¢æˆ·ç«¯æŒ‡ä»¤
    for client_name in client_names:
        if client_name not in client_instructions:
            client_instructions[client_name] = "æ— é¢å¤–ä¿¡æ¯"
    
    return server_instruction, client_instructions, sample['output']

def evaluate_test_accuracy(federated_model, test_samples: List[Dict], max_samples: int = 50) -> Dict:
    """åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¿«é€Ÿè¯„ä¼°æµ‹è¯•é›†å‡†ç¡®ç‡"""
    if not test_samples:
        return {'transport_accuracy': 0, 'city_accuracy': 0, 'province_accuracy': 0, 'overall_accuracy': 0, 'total_samples': 0}
    
    eval_samples = random.sample(test_samples, min(max_samples, len(test_samples)))
    
    correct_transport = 0
    correct_city = 0  
    correct_province = 0
    total_samples = len(eval_samples)
    
    federated_model.eval()
    with torch.no_grad():
        for sample in eval_samples:
            try:
                server_instruction, client_instructions, expected_output = parse_qwen_sample(sample)
                
                # ç”Ÿæˆé¢„æµ‹ç»“æœ
                generated_text = federated_model.generate(
                    server_instruction=server_instruction,
                    client_instructions=client_instructions,
                    max_new_tokens=100,
                    temperature=0.1
                )
                
                # ç®€å•çš„å‡†ç¡®ç‡è¯„ä¼°
                if "é“è·¯è¿è¾“" in expected_output and "é“è·¯è¿è¾“" in generated_text:
                    correct_transport += 1
                elif "å…¬è·¯è¿è¾“" in expected_output and "å…¬è·¯è¿è¾“" in generated_text:
                    correct_transport += 1
                    
            except Exception as e:
                logger.debug(f"è¯„ä¼°æ ·æœ¬å¤±è´¥: {e}")
                continue
    
    federated_model.train()
    
    return {
        'transport_accuracy': correct_transport / total_samples if total_samples > 0 else 0,
        'city_accuracy': 0,  # ç®€åŒ–ç‰ˆæœ¬
        'province_accuracy': 0,  # ç®€åŒ–ç‰ˆæœ¬
        'overall_accuracy': correct_transport / total_samples if total_samples > 0 else 0,
        'total_samples': total_samples
    }

def train_federated_qwen(
    data_dir: str = "./data",
    model_path: str = "/root/autodl-tmp/Federated_learning/llm_init_ckpt/Qwen2.5-0.5B-Instruct",
    epochs: int = 3,
    learning_rate: float = 2e-6,
    batch_size: int = 4,
    gradient_accumulation_steps: int = 2,
    max_length: int = 512,
    save_steps: int = 500,
    output_dir: str = "federated_qwen_output",
    max_samples: int = None,
    eval_steps: int = 1000
):
    """è®­ç»ƒè”é‚¦Qwenæ¨¡å‹"""
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"ğŸ“± ä½¿ç”¨è®¾å¤‡: {device}")
    
    try:
        # 1. åŠ è½½è”é‚¦æ¨¡å‹
        logger.info("ğŸ”§ åˆå§‹åŒ–è”é‚¦Qwenæ¨¡å‹...")
        federated_model = FederatedQwenSystem(model_path=model_path, device=device)
        federated_model.to(device)
        federated_model.train()
        
        logger.info("âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        
        # 2. åŠ è½½æ•°æ®
        logger.info("ğŸ“Š åŠ è½½è®­ç»ƒæ•°æ®...")
        # æ”¯æŒæ–°çš„å¢å¼ºç‰ˆæ•°æ®å’Œæ—§ç‰ˆæ•°æ®
        if (Path(data_dir) / "enhanced_qwen_train.jsonl").exists():
            train_file = Path(data_dir) / "enhanced_qwen_train.jsonl"
            test_file = Path(data_dir) / "enhanced_qwen_test.jsonl"
        elif (Path(data_dir) / "qwen_processed" / "qwen_federated_train.jsonl").exists():
            train_file = Path(data_dir) / "qwen_processed" / "qwen_federated_train.jsonl"
            test_file = Path(data_dir) / "qwen_processed" / "qwen_federated_test.jsonl"
        else:
            raise FileNotFoundError(f"åœ¨ {data_dir} ä¸­æ‰¾ä¸åˆ°è®­ç»ƒæ•°æ®æ–‡ä»¶")
        
        if not train_file.exists():
            raise FileNotFoundError(f"è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {train_file}")
            
        training_samples = load_training_data(train_file, max_samples)
        test_samples = []
        if test_file.exists():
            test_samples = load_training_data(test_file, 200)
        
        if len(training_samples) == 0:
            raise ValueError("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„è®­ç»ƒæ ·æœ¬")
        
        # 3. è®¾ç½®ä¼˜åŒ–å™¨
        optimizer = torch.optim.AdamW(
            federated_model.parameters(),
            lr=learning_rate,
            weight_decay=0.01
        )
        
        max_grad_norm = 1.0
        
        logger.info(f"ğŸš€ å¼€å§‹è®­ç»ƒ...")
        logger.info(f"   - è®­ç»ƒæ ·æœ¬æ•°: {len(training_samples)}")
        logger.info(f"   - è®­ç»ƒè½®æ•°: {epochs}")
        logger.info(f"   - å­¦ä¹ ç‡: {learning_rate}")
        logger.info(f"   - æ‰¹æ¬¡å¤§å°: {batch_size}")
        logger.info(f"   - æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {gradient_accumulation_steps}")
        
        # 4. è®­ç»ƒå¾ªç¯
        total_loss = 0.0
        valid_steps = 0
        global_step = 0
        
        for epoch in range(epochs):
            epoch_loss = 0.0
            epoch_valid_steps = 0
            
            progress_bar = tqdm(training_samples, desc=f"Epoch {epoch+1}")
            
            for step, sample in enumerate(progress_bar):
                try:
                    # è§£ææ ·æœ¬
                    server_instruction, client_instructions, expected_output = parse_qwen_sample(sample)
                    
                    # å‰å‘ä¼ æ’­
                    outputs = federated_model(
                        server_instruction=server_instruction,
                        client_instructions=client_instructions,
                        target_output=expected_output
                    )
                    
                    # æ£€æŸ¥æŸå¤±
                    if "loss" in outputs:
                        loss = outputs["loss"]
                        
                        if torch.isnan(loss) or torch.isinf(loss):
                            logger.warning(f"è·³è¿‡æ— æ•ˆæŸå¤±: {loss.item()}")
                            continue
                        
                        # æ¢¯åº¦ç´¯ç§¯
                        loss = loss / gradient_accumulation_steps
                        loss.backward()
                        
                        # æ£€æŸ¥æ˜¯å¦æ›´æ–°å‚æ•°
                        if (step + 1) % gradient_accumulation_steps == 0:
                # æ¢¯åº¦è£å‰ª
                            torch.nn.utils.clip_grad_norm_(federated_model.parameters(), max_grad_norm)
                
                            # æ›´æ–°å‚æ•°
                            optimizer.step()
                
                            # è”é‚¦å­¦ä¹ åŒæ­¥
                            federated_model.federated_step()
                            
                            optimizer.zero_grad()
                            
                            # å®šæœŸè¯„ä¼°
                            if test_samples and global_step > 0 and global_step % eval_steps == 0:
                                logger.info(f"\nğŸ“Š æ­¥éª¤ {global_step}: å¼€å§‹æµ‹è¯•é›†è¯„ä¼°...")
                                eval_results = evaluate_test_accuracy(federated_model, test_samples, max_samples=50)
                                logger.info(f"ğŸ“ˆ æµ‹è¯•é›†å‡†ç¡®ç‡: {eval_results['overall_accuracy']:.2%}")
                            
                            global_step += 1
                        
                        # è®°å½•æŸå¤±
                        loss_item = loss.item()
                        total_loss += loss_item
                        epoch_loss += loss_item
                        valid_steps += 1
                        epoch_valid_steps += 1
                        
                        # æ›´æ–°è¿›åº¦æ¡
                        avg_loss = total_loss / valid_steps
                        progress_bar.set_postfix({
                            'loss': f'{loss_item:.4f}',
                            'avg_loss': f'{avg_loss:.4f}',
                            'valid_steps': valid_steps
                        })
                
                except Exception as e:
                    logger.warning(f"å¤„ç†æ ·æœ¬å¤±è´¥: {e}")
                    continue
            
            # Epochç»“æŸ
            if epoch_valid_steps > 0:
                avg_epoch_loss = epoch_loss / epoch_valid_steps
                logger.info(f"âœ… Epoch {epoch+1} å®Œæˆ, å¹³å‡æŸå¤±: {avg_epoch_loss:.4f}")
        
        # 5. ä¿å­˜æ¨¡å‹
        logger.info("ğŸ’¾ ä¿å­˜è®­ç»ƒåçš„æ¨¡å‹...")
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        # ä¿å­˜æ¨¡å‹ï¼ˆæ ‡å‡†æ ¼å¼ï¼‰
        checkpoint = {
            'model_state_dict': federated_model.state_dict(),
            'epoch': epochs,
            'learning_rate': learning_rate,
            'batch_size': batch_size,
            'gradient_accumulation_steps': gradient_accumulation_steps,
            'total_loss': total_loss,
            'valid_steps': valid_steps,
            'avg_loss': total_loss / valid_steps if valid_steps > 0 else 0
        }
        torch.save(checkpoint, output_path / "federated_qwen_model.pth")
        logger.info(f"æ¨¡å‹æ£€æŸ¥ç‚¹å·²ä¿å­˜åˆ°: {output_path / 'federated_qwen_model.pth'}")
        
        # ä¿å­˜è®­ç»ƒé…ç½®
        config = {
            'epochs': epochs,
            'learning_rate': learning_rate,
            'batch_size': batch_size,
            'gradient_accumulation_steps': gradient_accumulation_steps,
            'total_loss': total_loss,
            'valid_steps': valid_steps,
            'avg_loss': total_loss / valid_steps if valid_steps > 0 else 0
        }
        
        with open(output_path / "training_config.json", 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        
        logger.info(f"âœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜åˆ°: {output_dir}")
        logger.info(f"   - æœ‰æ•ˆè®­ç»ƒæ­¥éª¤: {valid_steps}")
        logger.info(f"   - æœ€ç»ˆå¹³å‡æŸå¤±: {total_loss / valid_steps if valid_steps > 0 else 0:.4f}")
    
        return True
        
    except Exception as e:
        logger.error(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="è®­ç»ƒè”é‚¦Qwenæ¨¡å‹")
    parser.add_argument("--data_dir", type=str, default="./data", help="æ•°æ®ç›®å½•")
    parser.add_argument("--model_path", type=str, 
                       default="/root/autodl-tmp/Federated_learning/llm_init_ckpt/Qwen2.5-0.5B-Instruct",
                       help="æ¨¡å‹è·¯å¾„")
    parser.add_argument("--epochs", type=int, default=3, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--learning_rate", type=float, default=2e-6, help="å­¦ä¹ ç‡")
    parser.add_argument("--batch_size", type=int, default=4, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--gradient_accumulation_steps", type=int, default=2, help="æ¢¯åº¦ç´¯ç§¯æ­¥æ•°")
    parser.add_argument("--max_length", type=int, default=1024, help="æœ€å¤§åºåˆ—é•¿åº¦")
    parser.add_argument("--save_steps", type=int, default=500, help="ä¿å­˜æ­¥æ•°")
    parser.add_argument("--eval_steps", type=int, default=1000, help="è¯„ä¼°æ­¥æ•°")
    parser.add_argument("--output_dir", type=str, default="federated_qwen_output", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--max_samples", type=int, default=None, help="æœ€å¤§æ ·æœ¬æ•°")
    
    args = parser.parse_args()
    
    # å¼€å§‹è®­ç»ƒ
    success = train_federated_qwen(
        data_dir=args.data_dir,
        model_path=args.model_path,
        epochs=args.epochs,
        learning_rate=args.learning_rate,
        batch_size=args.batch_size,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        max_length=args.max_length,
        save_steps=args.save_steps,
        output_dir=args.output_dir,
        max_samples=args.max_samples,
        eval_steps=args.eval_steps
    )
    
    if success:
        logger.info("ğŸ‰ è®­ç»ƒæˆåŠŸå®Œæˆï¼")
    else:
        logger.error("âŒ è®­ç»ƒå¤±è´¥ï¼")
        exit(1)

if __name__ == "__main__":
    main()
