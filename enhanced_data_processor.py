# /********************************************************************************
#  * @Author: zhangqiuhong
#  * @Date: 2025-09-20
#  * @Description: å¢å¼ºç‰ˆè”é‚¦å­¦ä¹ æ•°æ®å¤„ç†å™¨ - ä½¿ç”¨é«˜è´¨é‡å®Œæ•´æ•°æ®æº
#  ********************************************************************************/

import pandas as pd
import numpy as np
import json
import logging
import argparse
import random
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from tqdm import tqdm

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class EnhancedFederatedDataProcessor:
    """å¢å¼ºç‰ˆè”é‚¦å­¦ä¹ æ•°æ®å¤„ç†å™¨ - ä½¿ç”¨é«˜è´¨é‡å®Œæ•´æ•°æ®æº"""

    def __init__(self, data_dir: str):
        self.data_dir = Path(data_dir)
        self.main_data_file = self.data_dir / 'æ½œåœ¨ç®±æº_æµ·å…³æ¨¡æ‹Ÿ_æ ·ä¾‹å…¨.csv'
        logger.info(f"ğŸ“ æ•°æ®ç›®å½•: {self.data_dir}")
        logger.info(f"ğŸ¯ ä½¿ç”¨é«˜è´¨é‡æ•°æ®æº: {self.main_data_file.name}")

    def load_enhanced_data(self) -> pd.DataFrame:
        """åŠ è½½å¢å¼ºç‰ˆå®Œæ•´æ•°æ®"""
        try:
            if not self.main_data_file.exists():
                raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {self.main_data_file}")

            logger.info("ğŸ“Š åŠ è½½é«˜è´¨é‡å®Œæ•´æ•°æ®...")
            df = pd.read_csv(self.main_data_file, encoding='utf-8')
            logger.info(f"âœ… æˆåŠŸåŠ è½½æ•°æ®: {len(df)} è¡Œ, {len(df.columns)} åˆ—")

            # æ•°æ®è´¨é‡æ£€æŸ¥
            self._check_data_quality(df)

            return df

        except Exception as e:
            logger.error(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            raise

    def _check_data_quality(self, df: pd.DataFrame):
        """æ£€æŸ¥æ•°æ®è´¨é‡"""
        logger.info("ğŸ” æ£€æŸ¥æ•°æ®è´¨é‡...")

        key_columns = ['CNTR', 'PM', 'FZHZM',
                       'DZHZM', 'ç›®çš„åœ°çœä»½', 'ç›®çš„åœ°åŸå¸‚', 'CARGO_WGT']

        quality_report = {}
        for col in key_columns:
            if col in df.columns:
                non_null_ratio = df[col].notna().mean()
                unique_count = df[col].nunique()
                quality_report[col] = {
                    'non_null_ratio': non_null_ratio,
                    'unique_count': unique_count
                }
                logger.info(
                    f"  {col}: éç©ºç‡={non_null_ratio:.2%}, å”¯ä¸€å€¼={unique_count}")
            else:
                logger.warning(f"  âš ï¸  å…³é”®åˆ—ç¼ºå¤±: {col}")

        return quality_report

    def create_enhanced_samples(self, df: pd.DataFrame, sample_size: int = 2000,
                                railway_ratio: float = 0.6) -> List[Dict]:
        """åˆ›å»ºå¢å¼ºç‰ˆè”é‚¦æ ·æœ¬ï¼Œæ§åˆ¶è¿è¾“æ–¹å¼æ¯”ä¾‹"""
        logger.info(f"ğŸš€ å¼€å§‹åˆ›å»º {sample_size} ä¸ªå¢å¼ºç‰ˆè”é‚¦æ ·æœ¬...")
        logger.info(
            f"ğŸ“Š è¿è¾“æ–¹å¼æ¯”ä¾‹è®¾ç½®: é“è·¯è¿è¾“ {railway_ratio:.1%}, éé“è·¯è¿è¾“ {1-railway_ratio:.1%}")

        # è¿‡æ»¤æœ‰æ•ˆæ•°æ®
        valid_df = self._filter_valid_data(df)
        logger.info(f"ğŸ“Š è¿‡æ»¤åæœ‰æ•ˆæ•°æ®: {len(valid_df)} è¡Œ")

        if len(valid_df) < sample_size:
            logger.warning(f"âš ï¸  æœ‰æ•ˆæ•°æ®ä¸è¶³ï¼Œè°ƒæ•´æ ·æœ¬æ•°é‡: {len(valid_df)}")
            sample_size = len(valid_df)

        # è®¡ç®—å„è¿è¾“æ–¹å¼çš„æ ·æœ¬æ•°é‡
        railway_samples = int(sample_size * railway_ratio)
        non_railway_samples = sample_size - railway_samples

        logger.info(
            f"ğŸ“Š æ ·æœ¬åˆ†é…: é“è·¯è¿è¾“ {railway_samples} ä¸ª, éé“è·¯è¿è¾“ {non_railway_samples} ä¸ª")

        # éšæœºé‡‡æ ·
        sampled_df = valid_df.sample(
            n=sample_size, random_state=42).reset_index(drop=True)

        # åˆ›å»ºè”é‚¦æ ·æœ¬
        samples = []
        railway_count = 0
        non_railway_count = 0

        for idx, row in tqdm(sampled_df.iterrows(), total=len(sampled_df), desc="åˆ›å»ºå¢å¼ºæ ·æœ¬"):
            try:
                # æ ¹æ®æ¯”ä¾‹æ§åˆ¶è¿è¾“æ–¹å¼
                if railway_count < railway_samples:
                    transport_mode = 'railway'
                    railway_count += 1
                elif non_railway_count < non_railway_samples:
                    transport_mode = 'non_railway'
                    non_railway_count += 1
                else:
                    # å¦‚æœä¸¤ç§æ–¹å¼éƒ½å·²æ»¡é¢ï¼Œéšæœºé€‰æ‹©
                    transport_mode = 'railway' if random.random() < 0.5 else 'non_railway'

                sample = self._create_single_sample(row, idx, transport_mode)
                if sample:
                    samples.append(sample)
            except Exception as e:
                logger.warning(f"æ ·æœ¬ {idx} åˆ›å»ºå¤±è´¥: {e}")
                continue

        # ç»Ÿè®¡æœ€ç»ˆçš„è¿è¾“æ–¹å¼åˆ†å¸ƒ
        railway_final = sum(
            1 for s in samples if '**è¿è¾“æ–¹å¼é€‰æ‹©**: é“è·¯è¿è¾“' in s.get('output', ''))
        non_railway_final = sum(
            1 for s in samples if '**è¿è¾“æ–¹å¼é€‰æ‹©**: å…¬è·¯è¿è¾“' in s.get('output', ''))
        other_final = len(samples) - railway_final - non_railway_final

        logger.info(f"âœ… æˆåŠŸåˆ›å»º {len(samples)} ä¸ªå¢å¼ºç‰ˆè”é‚¦æ ·æœ¬")
        logger.info(f"ğŸ“Š æœ€ç»ˆè¿è¾“æ–¹å¼åˆ†å¸ƒ: é“è·¯è¿è¾“ {railway_final} ({railway_final/len(samples):.1%}), "
                    f"éé“è·¯è¿è¾“ {non_railway_final} ({non_railway_final/len(samples):.1%})")

        return samples

    def _filter_valid_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """è¿‡æ»¤æœ‰æ•ˆæ•°æ®"""
        logger.info("ğŸ” è¿‡æ»¤æœ‰æ•ˆæ•°æ®...")

        # åŸºæœ¬è¿‡æ»¤æ¡ä»¶
        conditions = [
            df['CNTR'].notna(),
            df['PM'].notna(),
            df['FZHZM'].notna(),
            df['DZHZM'].notna(),
            df['ç›®çš„åœ°çœä»½'].notna(),
            df['ç›®çš„åœ°åŸå¸‚'].notna(),
            df['CARGO_WGT'].notna(),
            df['CARGO_WGT'] > 0
        ]

        # ç»„åˆæ‰€æœ‰æ¡ä»¶
        valid_mask = pd.concat(conditions, axis=1).all(axis=1)
        valid_df = df[valid_mask].copy()

        # é¢å¤–çš„è´¨é‡è¿‡æ»¤
        valid_df = valid_df[
            (valid_df['ç›®çš„åœ°çœä»½'] != 'æœªçŸ¥') &
            (valid_df['ç›®çš„åœ°åŸå¸‚'] != 'æœªçŸ¥') &
            (valid_df['PM'] != '') &
            (valid_df['FZHZM'] != '') &
            (valid_df['DZHZM'] != '')
        ]

        logger.info(
            f"ğŸ“Š è¿‡æ»¤ç»“æœ: {len(df)} â†’ {len(valid_df)} è¡Œ (ä¿ç•™ç‡: {len(valid_df)/len(df):.2%})")
        return valid_df

    def _create_single_sample(self, row: pd.Series, sample_idx: int, transport_mode: str = 'railway') -> Optional[Dict]:
        """åˆ›å»ºå•ä¸ªå¢å¼ºæ ·æœ¬"""
        try:
            # æ¸¯å£ä¸“å®¶ä¿¡æ¯
            port_text = self._create_port_expert_text(row)

            # é“è·¯ä¸“å®¶ä¿¡æ¯
            railway_text = self._create_railway_expert_text(
                row, transport_mode)

            # æµ·å…³ä¸“å®¶ä¿¡æ¯
            customs_text = self._create_customs_expert_text(row)

            # å†³ç­–è¾“å‡º
            output_text = self._create_decision_output(row, transport_mode)

            return {
                'sample_id': f"enhanced_{sample_idx:06d}",
                'port_text': port_text,
                'railway_text': railway_text,
                'customs_text': customs_text,
                'output': output_text,
                'cntr': row.get('CNTR', ''),
                'cargo_type': row.get('PM', ''),
                'origin': row.get('FZHZM', ''),
                'destination': row.get('DZHZM', ''),
                'dest_province': row.get('ç›®çš„åœ°çœä»½', ''),
                'dest_city': row.get('ç›®çš„åœ°åŸå¸‚', '')
            }

        except Exception as e:
            logger.warning(f"åˆ›å»ºæ ·æœ¬å¤±è´¥: {e}")
            return None

    def _create_port_expert_text(self, row: pd.Series) -> str:
        """åˆ›å»ºæ¸¯å£ä¸“å®¶æ–‡æœ¬"""
        cargo_type = row.get('PM', 'æœªçŸ¥è´§ç‰©')
        cargo_weight = row.get('CARGO_WGT', 0)
        container_size = row.get('CNTR_SIZ_COD', '20')
        container_type = row.get('CNTR_TYP_COD', 'GP')
        load_port = row.get('LOAD_PORT_COD', 'CNNSA')
        disc_port = row.get('DISC_PORT_COD', 'CNNSA')
        dest_port = row.get('DEST_PORT_COD', 'CNXIN')

        # æ¸¯å£ä½œä¸šæ—¶é—´ä¿¡æ¯ï¼ˆæ™ºèƒ½é€‰æ‹©æ•°æ®æºï¼‰
        berth_time = None

        # ä¼˜å…ˆä½¿ç”¨WAIT_HOURSå­—æ®µï¼ˆæ½œåœ¨ç®±æºæ•°æ®ï¼‰
        if 'WAIT_HOURS' in row and pd.notna(row.get('WAIT_HOURS')):
            berth_time = float(row['WAIT_HOURS']) * \
                random.uniform(0.8, 1.2)  # æ·»åŠ Â±20%éšæœºæ€§
        # å¤‡é€‰ä½¿ç”¨ETA_to_BerthTimeå­—æ®µï¼ˆæ¸¯å£æ•°æ®ï¼‰
        elif 'ETA_to_BerthTime' in row and pd.notna(row.get('ETA_to_BerthTime')):
            eta_berth = float(row['ETA_to_BerthTime'])
            # å¤„ç†è´Ÿå€¼å’Œå¼‚å¸¸å€¼
            if eta_berth > 0 and eta_berth < 100:
                berth_time = eta_berth * random.uniform(0.8, 1.2)

        # å¦‚æœéƒ½æ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼Œä½¿ç”¨åˆç†çš„éšæœºå€¼
        if berth_time is None or berth_time <= 0 or berth_time > 100:
            berth_time = random.uniform(8, 48)

        # å †åœºåœç•™æ—¶é—´ï¼šæ ¹æ®è´§ç‰©ç±»å‹å’Œæ¸¯å£ä½œä¸šæƒ…å†µéšæœºç”Ÿæˆ
        yard_stay = random.uniform(24, 72)

        port_text = f"""<|æ¸¯å£ä¸“å®¶|>
ä½œä¸ºæ¸¯å£è¿è¥ä¸“å®¶ï¼Œæˆ‘æä¾›ä»¥ä¸‹æ¸¯å£ä½œä¸šå’Œè´§ç‰©ä¿¡æ¯ï¼š
è´§ç‰©ç±»å‹ï¼š{cargo_type}ã€‚è£…ç®±æ–¹å¼ï¼š{container_size}å°ºå¯¸{container_type}å‹é›†è£…ç®±ã€‚è´¸æ˜“æ–¹å‘ï¼šè¿›å£ã€‚ç›®çš„æ¸¯ï¼š{dest_port}æ¸¯ã€‚æ¸¯å£ä½œä¸šï¼šé æ³Šæ­£å¸¸ï¼Œç­‰å¾…æ—¶é—´{berth_time:.1f}å°æ—¶ã€‚å †åœºæƒ…å†µï¼šå †åœºå‘¨è½¬æ­£å¸¸ï¼Œåœç•™{yard_stay:.1f}å°æ—¶ã€‚è´§ç‰©é‡é‡ï¼š{cargo_weight}å…¬æ–¤ã€‚è£…è´§æ¸¯ï¼š{load_port}ï¼Œå¸è´§æ¸¯ï¼š{disc_port}ã€‚"""

        return port_text

    def _create_railway_expert_text(self, row: pd.Series, transport_mode: str = 'railway') -> str:
        """åˆ›å»ºé“è·¯ä¸“å®¶æ–‡æœ¬"""
        origin_station = row.get('FZHZM', 'å—æ²™æ¸¯')
        dest_station = row.get('DZHZM', 'ä¸‰åŸ')
        cargo_type = row.get('PM', 'é¢ç²‰')

        if transport_mode == 'railway':
            # é“è·¯è¿è¾“æ¨¡å¼
            train_no = row.get('XH', f"G{random.randint(1000, 9999)}")
            transit_hours = row.get('TRANSIT_HOURS', 24)
            rail_price = row.get('95306_PRICE', 3500)

            railway_text = f"""<|é“è·¯ä¸“å®¶|>
ä½œä¸ºé“è·¯è¿è¾“ä¸“å®¶ï¼Œæˆ‘æä¾›ä»¥ä¸‹é“è·¯è¿è¾“ä¿¡æ¯ï¼š
åˆ—è½¦è½¦æ¬¡ï¼š{train_no}ï¼Œå‘ç«™ï¼š{origin_station}ï¼Œåˆ°ç«™ï¼š{dest_station}ã€‚è´§ç‰©å“åï¼š{cargo_type}ã€‚é¢„è®¡è¿è¾“æ—¶é—´ï¼š{transit_hours:.1f}å°æ—¶ã€‚é“è·¯è¿ä»·ï¼š{rail_price}å…ƒã€‚è¿è¾“çŠ¶æ€ï¼šæ­£å¸¸è¿è¡Œï¼Œæ— å»¶è¯¯ã€‚çº¿è·¯æ¡ä»¶ï¼šå¹²çº¿ç›´è¾¾ï¼Œè¿èƒ½å……è¶³ã€‚"""
        else:
            # éé“è·¯è¿è¾“æ¨¡å¼ï¼ˆå…¬è·¯è¿è¾“ï¼‰
            distance = random.randint(300, 1200)  # å…¬è·¯è·ç¦»
            road_hours = distance / random.randint(60, 80)  # æŒ‰é€Ÿåº¦è®¡ç®—æ—¶é—´
            road_price = distance * random.uniform(2.5, 4.0)  # å…¬è·¯è¿ä»·
            transit_hours = row.get('TRANSIT_HOURS', 24)  # è·å–é“è·¯è¿è¾“æ—¶é—´ä½œä¸ºå¯¹æ¯”

            railway_text = f"""<|é“è·¯ä¸“å®¶|>
ä½œä¸ºé“è·¯è¿è¾“ä¸“å®¶ï¼Œæˆ‘æä¾›ä»¥ä¸‹é“è·¯è¿è¾“åˆ†æï¼š
é“è·¯çº¿è·¯ï¼š{origin_station}è‡³{dest_station}çº¿è·¯å½“å‰è¿èƒ½ç´§å¼ ã€‚è´§ç‰©å“åï¼š{cargo_type}ã€‚é“è·¯è¿è¾“æ—¶æ•ˆï¼šçº¦{transit_hours:.1f}å°æ—¶ï¼Œä½†å‘è½¦ç­æ¬¡æœ‰é™ã€‚å»ºè®®è€ƒè™‘å…¬è·¯è¿è¾“æ›¿ä»£æ–¹æ¡ˆï¼šé¢„è®¡å…¬è·¯è·ç¦»{distance}å…¬é‡Œï¼Œè¿è¾“æ—¶é—´{road_hours:.1f}å°æ—¶ï¼Œè¿ä»·çº¦{road_price:.0f}å…ƒã€‚"""

        return railway_text

    def _create_customs_expert_text(self, row: pd.Series) -> str:
        """åˆ›å»ºæµ·å…³ä¸“å®¶æ–‡æœ¬"""
        clearance_time = row.get('customs_clearance_time_days', 2)
        risk_level = row.get('customs_risk_level', 'LOW')
        inspection_prob = row.get('inspection_probability', 0.1)
        dest_province = row.get('ç›®çš„åœ°çœä»½', 'é™•è¥¿çœ')
        dest_city = row.get('ç›®çš„åœ°åŸå¸‚', 'å’¸é˜³å¸‚')

        risk_level_cn = {'LOW': 'ä½é£é™©', 'MEDIUM': 'ä¸­é£é™©',
                         'HIGH': 'é«˜é£é™©'}.get(risk_level, 'ä½é£é™©')

        customs_text = f"""<|æµ·å…³ä¸“å®¶|>
ä½œä¸ºæµ·å…³ä¸šåŠ¡ä¸“å®¶ï¼Œæˆ‘æä¾›ä»¥ä¸‹æµ·å…³æ¸…å…³ä¿¡æ¯ï¼š
æ¸…å…³çŠ¶æ€ï¼šæ­£å¸¸é€šå…³ã€‚é¢„è®¡æ¸…å…³æ—¶é—´ï¼š{clearance_time}å¤©ã€‚é£é™©ç­‰çº§ï¼š{risk_level_cn}ã€‚æŸ¥éªŒæ¦‚ç‡ï¼š{inspection_prob:.1%}ã€‚ç›®çš„åœ°ï¼š{dest_province}{dest_city}ã€‚è´¸æ˜“åˆè§„ï¼šç¬¦åˆç›¸å…³æ³•è§„è¦æ±‚ã€‚å•è¯é½å…¨ï¼šè¿›å£è®¸å¯ã€åŸäº§åœ°è¯æ˜ç­‰å•è¯å®Œå¤‡ã€‚"""

        return customs_text

    def _create_decision_output(self, row: pd.Series, transport_mode: str = 'railway') -> str:
        """åˆ›å»ºå†³ç­–è¾“å‡º"""
        dest_province = row.get('ç›®çš„åœ°çœä»½', 'é™•è¥¿çœ')
        dest_city = row.get('ç›®çš„åœ°åŸå¸‚', 'å’¸é˜³å¸‚')
        cargo_type = row.get('PM', 'é¢ç²‰')
        origin_station = row.get('FZHZM', 'å—æ²™æ¸¯')
        dest_station = row.get('DZHZM', 'ä¸‰åŸ')

        # æ ¹æ®ä¼ å…¥çš„è¿è¾“æ–¹å¼å‚æ•°ç¡®å®šè¿è¾“æ–¹å¼
        if transport_mode == 'railway':
            transport_mode_cn = "é“è·¯è¿è¾“"
            mode_reason = "åŸºäºé“è·¯ç›´è¾¾çº¿è·¯å’Œæˆæœ¬ä¼˜åŠ¿"
            optimization_detail = f"{origin_station}è‡³{dest_station}é“è·¯ç›´è¾¾ï¼Œè¿è¾“æ•ˆç‡é«˜"
        else:
            transport_mode_cn = "å…¬è·¯è¿è¾“"
            mode_reason = "åŸºäºçµæ´»æ€§å’Œæ—¶æ•ˆæ€§è€ƒè™‘"
            optimization_detail = f"{origin_station}è‡³{dest_province}{dest_city}å…¬è·¯è¿è¾“ï¼Œé—¨åˆ°é—¨æœåŠ¡"

        output = f"""åŸºäºå¤šæºä¿¡æ¯ç»¼åˆåˆ†æï¼š

**è¿è¾“æ–¹å¼é€‰æ‹©**: {transport_mode_cn}
**ç›®çš„åœ°åŸå¸‚**: {dest_city}
**ç›®çš„åœ°çœä»½**: {dest_province}

**å†³ç­–ç†ç”±**:
1. **å¤šæºæ•°æ®èåˆ**: ç»¼åˆæ¸¯å£ä½œä¸šæ•ˆç‡ã€é“è·¯è¿è¾“æˆæœ¬ã€æµ·å…³æ¸…å…³æ—¶æ•ˆç­‰å…³é”®å› ç´ 
2. **è¿è¾“æ–¹å¼ä¼˜é€‰**: é€‰æ‹©{transport_mode_cn}{mode_reason}
3. **ç›®çš„åœ°é¢„æµ‹**: æ ¹æ®è´§ç‰©ç‰¹å¾å’Œè´¸æ˜“æµå‘ï¼Œç¡®å®šç›®çš„åœ°ä¸º{dest_province}{dest_city}
4. **è´§ç‰©é€‚é…**: {cargo_type}ç±»è´§ç‰©é€‚åˆå½“å‰è¿è¾“æ–¹æ¡ˆ
5. **è·¯å¾„ä¼˜åŒ–**: {optimization_detail}"""

        return output


class EnhancedQwenFormatter:
    """å¢å¼ºç‰ˆQwenæ ¼å¼åŒ–å™¨"""

    def __init__(self):
        self.separator = " <|object_ref_start|> "
        self.end_separator = " <|object_ref_end|> "

    def format_to_qwen(self, sample: Dict) -> Dict[str, str]:
        """è½¬æ¢ä¸ºQwenè®­ç»ƒæ ¼å¼"""
        try:
            # æ„å»ºinstruction
            server_instruction = "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ç‰©æµå†³ç­–ç³»ç»Ÿã€‚åŸºäºå¤šä¸ªå®¢æˆ·ç«¯æä¾›çš„ä¸“ä¸šä¿¡æ¯ï¼Œè¯·ç»¼åˆåˆ†æå¹¶åšå‡ºæœ€ä¼˜çš„è¿è¾“å†³ç­–ã€‚\n\nè¯·æ ¹æ®ä»¥ä¸‹å¤šæºæ•°æ®ä¿¡æ¯ï¼Œé€‰æ‹©æœ€é€‚åˆçš„è¿è¾“æ–¹å¼ï¼Œé¢„æµ‹ç›®çš„åœ°ï¼Œå¹¶æä¾›è¯¦ç»†çš„å†³ç­–ç†ç”±ï¼š"

            instruction = (
                server_instruction +
                self.separator + sample['port_text'] + self.end_separator +
                self.separator + sample['railway_text'] + self.end_separator +
                self.separator + sample['customs_text'] + self.end_separator
            )

            return {
                "instruction": instruction,
                "input": "",
                "output": sample['output'],
                "sample_id": sample['sample_id']
            }
        except Exception as e:
            logger.error(f"æ ¼å¼åŒ–æ ·æœ¬å¤±è´¥: {e}")
            return None


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="å¢å¼ºç‰ˆè”é‚¦å­¦ä¹ æ•°æ®å¤„ç†")
    parser.add_argument("--data_dir", type=str,
                        default="/root/autodl-tmp/Federated_learning/code_v01/verify_data",
                        help="åŸå§‹æ•°æ®ç›®å½•")
    parser.add_argument("--output_dir", type=str, default="./data/enhanced_processed",
                        help="è¾“å‡ºç›®å½•")
    parser.add_argument("--sample_size", type=int, default=2000,
                        help="ç”Ÿæˆæ ·æœ¬æ•°é‡")
    parser.add_argument("--train_ratio", type=float, default=0.8,
                        help="è®­ç»ƒé›†æ¯”ä¾‹ (é»˜è®¤: 0.8)")
    parser.add_argument("--random_seed", type=int, default=42,
                        help="éšæœºç§å­ (é»˜è®¤: 42)")
    parser.add_argument("--railway_ratio", type=float, default=0.6,
                        help="é“è·¯è¿è¾“æ¯”ä¾‹ (é»˜è®¤: 0.6ï¼Œå³60%é“è·¯è¿è¾“ï¼Œ40%å…¬è·¯è¿è¾“)")

    args = parser.parse_args()

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    try:
        logger.info("ğŸš€ å¼€å§‹å¢å¼ºç‰ˆè”é‚¦æ•°æ®å¤„ç†...")

        # 1. åˆå§‹åŒ–å¤„ç†å™¨
        processor = EnhancedFederatedDataProcessor(args.data_dir)

        # 2. åŠ è½½é«˜è´¨é‡æ•°æ®
        df = processor.load_enhanced_data()

        # 3. åˆ›å»ºå¢å¼ºæ ·æœ¬ï¼ˆæ§åˆ¶è¿è¾“æ–¹å¼æ¯”ä¾‹ï¼‰
        samples = processor.create_enhanced_samples(
            df, args.sample_size, args.railway_ratio)

        if not samples:
            logger.error("âŒ æ²¡æœ‰ç”Ÿæˆä»»ä½•æ ·æœ¬")
            return False

        # 4. æ ¼å¼åŒ–ä¸ºQwenæ ¼å¼
        logger.info("ğŸ“ è½¬æ¢ä¸ºQwenè®­ç»ƒæ ¼å¼...")
        formatter = EnhancedQwenFormatter()
        qwen_samples = []

        for sample in tqdm(samples, desc="æ ¼å¼åŒ–æ ·æœ¬"):
            qwen_sample = formatter.format_to_qwen(sample)
            if qwen_sample:
                qwen_samples.append(qwen_sample)

        logger.info(f"âœ… æˆåŠŸæ ¼å¼åŒ– {len(qwen_samples)} ä¸ªæ ·æœ¬")

        # 5. åˆ†å‰²è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        logger.info(f"ğŸ”€ åˆ†å‰²è®­ç»ƒé›†å’Œæµ‹è¯•é›†...")

        import random
        random.seed(args.random_seed)

        # éšæœºæ‰“ä¹±æ ·æœ¬
        shuffled_samples = qwen_samples.copy()
        random.shuffle(shuffled_samples)

        # è®¡ç®—åˆ†å‰²ç‚¹
        total_samples = len(shuffled_samples)
        train_size = int(total_samples * args.train_ratio)
        test_size = total_samples - train_size

        train_samples = shuffled_samples[:train_size]
        test_samples = shuffled_samples[train_size:]

        logger.info(f"ğŸ“Š æ•°æ®åˆ†å‰²ç»“æœ:")
        logger.info(f"   - æ€»æ ·æœ¬æ•°: {total_samples}")
        logger.info(f"   - è®­ç»ƒé›†: {len(train_samples)} ({args.train_ratio:.1%})")
        logger.info(
            f"   - æµ‹è¯•é›†: {len(test_samples)} ({1-args.train_ratio:.1%})")

        # 6. ä¿å­˜è®­ç»ƒé›†
        train_file = output_dir / "enhanced_qwen_train.jsonl"
        with open(train_file, 'w', encoding='utf-8') as f:
            for sample in train_samples:
                f.write(json.dumps(sample, ensure_ascii=False) + '\n')

        # 7. ä¿å­˜æµ‹è¯•é›†
        test_file = output_dir / "enhanced_qwen_test.jsonl"
        with open(test_file, 'w', encoding='utf-8') as f:
            for sample in test_samples:
                f.write(json.dumps(sample, ensure_ascii=False) + '\n')

        # 8. ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        stats = {
            'total_samples': total_samples,
            'train_samples': len(train_samples),
            'test_samples': len(test_samples),
            'train_ratio': args.train_ratio,
            'random_seed': args.random_seed,
            'data_source': processor.main_data_file.name,
            'processing_time': pd.Timestamp.now().isoformat()
        }

        stats_file = output_dir / "enhanced_processing_stats.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)

        logger.info(f"ğŸ’¾ æ•°æ®ä¿å­˜å®Œæˆ:")
        logger.info(f"   - è®­ç»ƒé›†: {train_file}")
        logger.info(f"   - æµ‹è¯•é›†: {test_file}")
        logger.info(f"   - ç»Ÿè®¡ä¿¡æ¯: {stats_file}")
        logger.info("ğŸ‰ å¢å¼ºç‰ˆæ•°æ®å¤„ç†å®Œæˆ!")

        return True

    except Exception as e:
        logger.error(f"âŒ æ•°æ®å¤„ç†å¤±è´¥: {e}")
        return False


if __name__ == "__main__":
    main()
