#!/usr/bin/env python3
# /********************************************************************************
#  * Split Learningæ ¸å¿ƒæ€æƒ³ï¼š
#  * 1. å®¢æˆ·ç«¯åªä¿ç•™embeddingå±‚ï¼Œè¿›è¡Œå‰å‘ä¼ æ’­è·å–embedding
#  * 2. å°†embeddingä¼ è¾“åˆ°æœåŠ¡ç«¯è¿›è¡Œåç»­è®¡ç®—å’Œæ¢¯åº¦æ›´æ–°
#  * 3. æœåŠ¡ç«¯æ›´æ–°embeddingæƒé‡ååˆ†å‘ç»™æ‰€æœ‰å®¢æˆ·ç«¯
#  * 4. å®¢æˆ·ç«¯ä¸è¿›è¡Œåå‘ä¼ æ’­ï¼Œåªè´Ÿè´£embeddingè®¡ç®—
#  ********************************************************************************/

import torch
import torch.nn as nn
from transformers import Qwen2ForCausalLM, Qwen2Tokenizer
from typing import Dict, List, Optional, Tuple
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.DEBUG,
                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class QwenClientEmbedding(nn.Module):
    """
    Split Learningå®¢æˆ·ç«¯åµŒå…¥å±‚
    - åªè´Ÿè´£å‰å‘ä¼ æ’­ï¼šæ–‡æœ¬ -> embedding
    - ä¸è¿›è¡Œåå‘ä¼ æ’­ï¼Œæƒé‡ç”±æœåŠ¡ç«¯æ›´æ–°ååŒæ­¥
    """

    def __init__(self,
                 client_type: str = "general",
                 model_path: str = "/root/autodl-tmp/Federated_learning/llm_init_ckpt/Qwen2.5-0.5B-Instruct",
                 max_length: int = 1024,
                 device: str = 'cuda'):
        super().__init__()

        self.client_type = client_type
        self.model_path = model_path
        self.max_length = max_length
        self.device = device

        logger.info(f"ğŸ”§ åˆå§‹åŒ– {client_type} å®¢æˆ·ç«¯åµŒå…¥å±‚...")

        # åŠ è½½tokenizer
        self.tokenizer = Qwen2Tokenizer.from_pretrained(
            model_path,
            trust_remote_code=True
        )

        # åŠ è½½å®Œæ•´æ¨¡å‹ä»¥æå–embeddingå±‚
        full_model = Qwen2ForCausalLM.from_pretrained(
            model_path,
            trust_remote_code=True,
            torch_dtype=torch.float32,  # ç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´
            device_map='cpu'
        )

        # åªä¿ç•™embeddingå±‚
        self.embed_tokens = nn.Embedding(
            full_model.model.embed_tokens.num_embeddings,
            full_model.model.embed_tokens.embedding_dim,
            padding_idx=full_model.model.embed_tokens.padding_idx
        )

        # å¤åˆ¶æƒé‡
        with torch.no_grad():
            self.embed_tokens.weight.copy_(
                full_model.model.embed_tokens.weight)

        # Split Learning: å®¢æˆ·ç«¯ä¸éœ€è¦æ¢¯åº¦è®¡ç®—
        self.embed_tokens.requires_grad_(False)

        # æ¸…ç†å®Œæ•´æ¨¡å‹ä»¥èŠ‚çœå†…å­˜
        del full_model
        torch.cuda.empty_cache()

        logger.info(f"âœ… {client_type}å®¢æˆ·ç«¯åµŒå…¥å±‚åˆå§‹åŒ–å®Œæˆ:")
        logger.info(f"   æ¨¡å‹è·¯å¾„: {model_path}")
        logger.info(f"   åµŒå…¥ç»´åº¦: {self.embed_tokens.embedding_dim}")
        logger.info(f"   è¯æ±‡è¡¨å¤§å°: {self.embed_tokens.num_embeddings}")
        logger.info(f"   æœ€å¤§é•¿åº¦: {max_length}")

    def update_embedding_weights(self, new_weights: torch.Tensor):
        """
        æ¥æ”¶æœåŠ¡ç«¯æ›´æ–°çš„embeddingæƒé‡
        è¿™æ˜¯Split Learningçš„æ ¸å¿ƒï¼šæœåŠ¡ç«¯è®­ç»ƒååˆ†å‘æƒé‡ç»™å®¢æˆ·ç«¯

        Args:
            new_weights: æœåŠ¡ç«¯æ›´æ–°åçš„embeddingæƒé‡
        """
        with torch.no_grad():
            self.embed_tokens.weight.copy_(new_weights)
        logger.debug(f"ğŸ”„ {self.client_type} å®¢æˆ·ç«¯embeddingæƒé‡å·²æ›´æ–°")

    def forward(self, input_text: str) -> torch.Tensor:
        """
        Split Learningå®¢æˆ·ç«¯å‰å‘ä¼ æ’­ï¼šæ–‡æœ¬ -> embedding
        å®¢æˆ·ç«¯åªè®¡ç®—embeddingï¼Œä¸è¿›è¡Œåç»­å¤„ç†

        Args:
            input_text: è¾“å…¥æ–‡æœ¬

        Returns:
            embeddings: åµŒå…¥å‘é‡ [seq_len, embedding_dim]
        """
        # Tokenize
        encoded = self.tokenizer(
            input_text,
            add_special_tokens=False,
            return_tensors='pt',
        )

        # # Debug: æ£€æŸ¥tokenizationåçš„ç»“æœ
        # decoded_text = self.tokenizer.decode(encoded['input_ids'][0], skip_special_tokens=False)
        # logger.debug(f"ğŸ” è§£ç åæ–‡æœ¬: {decoded_text}")

        input_ids = encoded['input_ids'].to(self.device)

        # Split Learning: å®¢æˆ·ç«¯åªè¿›è¡ŒembeddingæŸ¥æ‰¾ï¼Œä¸è®¡ç®—æ¢¯åº¦
        with torch.no_grad():
            # [1, seq_len, embedding_dim]
            embeddings = self.embed_tokens(input_ids)

        return embeddings.squeeze(0)  # [seq_len, embedding_dim]

    def to(self, device):
        """ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡"""
        super().to(device)
        self.device = device
        return self


class QwenServerModel(nn.Module):
    """QwenæœåŠ¡ç«¯æ¨¡å‹ - åŒ…å«å®Œæ•´çš„Qwenç»“æ„"""

    def __init__(self,
                 model_path: str = "/root/autodl-tmp/Federated_learning/llm_init_ckpt/Qwen2.5-0.5B-Instruct",
                 device: str = 'cuda'):
        super().__init__()

        self.model_path = model_path
        self.device = device

        logger.info("ğŸ”§ åˆå§‹åŒ–æœåŠ¡ç«¯Qwen2.5æ¨¡å‹...")

        # åŠ è½½å®Œæ•´çš„Qwenæ¨¡å‹
        self.model = Qwen2ForCausalLM.from_pretrained(
            model_path,
            trust_remote_code=True,
            torch_dtype=torch.float32,  # ç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´
            device_map='cpu'
        )

        # åŠ è½½tokenizer
        self.tokenizer = Qwen2Tokenizer.from_pretrained(
            model_path,
            trust_remote_code=True
        )

        # è·å–åˆ†éš”ç¬¦token IDs
        self.sep_start_token = self.tokenizer.encode(
            "<|object_ref_start|>", add_special_tokens=False)[0]
        self.sep_end_token = self.tokenizer.encode(
            "<|object_ref_end|>", add_special_tokens=False)[0]

        logger.info("âœ… æœåŠ¡ç«¯Qwen2.5æ¨¡å‹åˆå§‹åŒ–å®Œæˆ:")
        logger.info(f"   æ¨¡å‹è·¯å¾„: {model_path}")
        logger.info(f"   åµŒå…¥ç»´åº¦: {self.model.model.embed_tokens.embedding_dim}")
        logger.info(
            f"   è¯æ±‡è¡¨å¤§å°: {self.model.model.embed_tokens.num_embeddings}")
        logger.info(
            f"   åˆ†éš”ç¬¦tokens: {self.sep_start_token}, {self.sep_end_token}")

    def create_federated_input_embeddings(self,
                                          server_instruction: str,
                                          client_embeddings: Dict[str, torch.Tensor],
                                          target_output: str = None) -> Tuple[torch.Tensor, torch.Tensor, Optional[int], Optional[torch.Tensor]]:
        """
        Split Learning: åˆ›å»ºè”é‚¦è¾“å…¥embeddings (æ‰‹åŠ¨æ‹¼æ¥Qwenæ ¼å¼)

        Args:
            server_instruction: æœåŠ¡ç«¯æŒ‡ä»¤
            client_embeddings: å®¢æˆ·ç«¯embeddingså­—å…¸ {client_name: embeddings_tensor}
            target_output: ç›®æ ‡è¾“å‡ºæ–‡æœ¬ï¼ˆè®­ç»ƒæ—¶éœ€è¦æ‹¼æ¥åˆ°è¾“å…¥åé¢ï¼‰

        Returns:
            combined_embeddings: æ‹¼æ¥åçš„embeddings [1, total_seq_len, embed_dim]
            combined_attention_mask: æ‹¼æ¥åçš„attention mask [1, total_seq_len]
            output_start_pos: outputéƒ¨åˆ†å¼€å§‹çš„ä½ç½®ï¼ˆç”¨äºSFTæŸå¤±è®¡ç®—ï¼‰ï¼Œå¦‚æœæ²¡æœ‰target_outputåˆ™ä¸ºNone
            labels: å¯¹åº”çš„token IDsæ ‡ç­¾ [1, total_seq_len]ï¼Œå¦‚æœæ²¡æœ‰target_outputåˆ™ä¸ºNone
        """
        # 1. è·å–ç‰¹æ®Štokensçš„IDs
        im_start_token = self.tokenizer.convert_tokens_to_ids("<|im_start|>")
        im_end_token = self.tokenizer.convert_tokens_to_ids("<|im_end|>")

        all_embeddings = []
        all_masks = []
        all_token_ids = []  # åŒæ—¶æ„å»ºtoken IDsç”¨äºlabels

        # 2. æ·»åŠ æœåŠ¡ç«¯æŒ‡ä»¤éƒ¨åˆ†: <|im_start|>server\n{instruction}<|im_end|>
        # <|im_start|>
        im_start_ids = torch.tensor([[im_start_token]], device=self.device)
        im_start_emb = self.model.model.embed_tokens(im_start_ids)
        all_embeddings.append(im_start_emb)
        all_masks.append(torch.ones((1, 1), device=self.device))
        all_token_ids.append(im_start_token)

        # server\n
        server_role_text = "server\n" + server_instruction
        server_encoded = self.tokenizer(
            server_role_text,
            add_special_tokens=False,
            return_tensors='pt'
        )
        server_ids = server_encoded['input_ids'].to(self.device)
        server_emb = self.model.model.embed_tokens(server_ids)
        server_mask = server_encoded['attention_mask'].to(self.device)
        all_embeddings.append(server_emb)
        all_masks.append(server_mask)
        all_token_ids.extend(server_ids[0].tolist())

        # <|im_end|>
        im_end_ids = torch.tensor([[im_end_token]], device=self.device)
        im_end_emb = self.model.model.embed_tokens(im_end_ids)
        all_embeddings.append(im_end_emb)
        all_masks.append(torch.ones((1, 1), device=self.device))
        all_token_ids.append(im_end_token)

        # 3. æ·»åŠ æ¯ä¸ªå®¢æˆ·ç«¯çš„embedding: <|im_start|>client_name\n{client_embedding}<|im_end|>
        for client_name in sorted(client_embeddings.keys()):
            client_emb = client_embeddings[client_name]  # [seq_len, embed_dim]

            # <|im_start|>
            all_embeddings.append(im_start_emb)
            all_masks.append(torch.ones((1, 1), device=self.device))
            all_token_ids.append(im_start_token)

            # client_name\n
            client_role_text = f"{client_name}\n"
            client_role_encoded = self.tokenizer(
                client_role_text,
                add_special_tokens=False,
                return_tensors='pt'
            )
            client_role_ids = client_role_encoded['input_ids'].to(self.device)
            client_role_emb = self.model.model.embed_tokens(client_role_ids)
            client_role_mask = client_role_encoded['attention_mask'].to(
                self.device)
            all_embeddings.append(client_role_emb)
            all_masks.append(client_role_mask)
            all_token_ids.extend(client_role_ids[0].tolist())

            # å®¢æˆ·ç«¯embeddingï¼ˆæ·»åŠ batchç»´åº¦ï¼‰
            client_emb_batched = client_emb.unsqueeze(
                0)  # [1, seq_len, embed_dim]
            client_mask = torch.ones(
                (1, client_emb.size(0)), device=self.device)
            all_embeddings.append(client_emb_batched)
            all_masks.append(client_mask)
            # æ³¨æ„ï¼šå®¢æˆ·ç«¯å†…å®¹çš„token IDsæˆ‘ä»¬æ— æ³•é‡å»ºï¼Œç”¨å ä½ç¬¦ï¼ˆè¿™éƒ¨åˆ†ä¸è®¡ç®—æŸå¤±ï¼‰
            all_token_ids.extend([0] * client_emb.size(0))

            # <|im_end|>
            all_embeddings.append(im_end_emb)
            all_masks.append(torch.ones((1, 1), device=self.device))
            all_token_ids.append(im_end_token)

        # 4. å¦‚æœæœ‰target_outputï¼Œæ·»åŠ assistantéƒ¨åˆ†
        output_start_pos = None
        if target_output is not None:
            # <|im_start|>
            all_embeddings.append(im_start_emb)
            all_masks.append(torch.ones((1, 1), device=self.device))
            all_token_ids.append(im_start_token)

            # assistant\n (è§’è‰²æ ‡è¯†ç¬¦ï¼Œä¸è®¡ç®—æŸå¤±)
            assistant_role_text = "assistant\n"
            assistant_role_encoded = self.tokenizer(
                assistant_role_text,
                add_special_tokens=False,
                return_tensors='pt'
            )
            assistant_role_ids = assistant_role_encoded['input_ids'].to(
                self.device)
            assistant_role_emb = self.model.model.embed_tokens(
                assistant_role_ids)
            assistant_role_mask = assistant_role_encoded['attention_mask'].to(
                self.device)
            all_embeddings.append(assistant_role_emb)
            all_masks.append(assistant_role_mask)
            all_token_ids.extend(assistant_role_ids[0].tolist())

            # è®°å½•å®é™…è¾“å‡ºå†…å®¹å¼€å§‹ä½ç½®ï¼ˆæ’é™¤<|im_start|>assistant\néƒ¨åˆ†ï¼‰
            output_start_pos = len(all_token_ids)

            # æ£€æŸ¥æ˜¯å¦æ˜¯æ¨ç†æ¨¡å¼
            if target_output == "INFERENCE_MODE":
                # æ¨ç†æ¨¡å¼ï¼šåªæ·»åŠ <|im_start|>assistant\nï¼Œä¸æ·»åŠ å…·ä½“å†…å®¹å’Œ<|im_end|>
                # æ¨¡å‹å°†ä»è¿™é‡Œå¼€å§‹ç”Ÿæˆ
                pass  # ä¸æ·»åŠ ä»»ä½•å†…å®¹ï¼Œè®©æ¨¡å‹è‡ªå·±ç”Ÿæˆ
            else:
                # è®­ç»ƒæ¨¡å¼ï¼šæ·»åŠ å®Œæ•´çš„target_outputå†…å®¹
                # {target_output} (å®é™…è¾“å‡ºå†…å®¹ï¼Œéœ€è¦è®¡ç®—æŸå¤±)
                output_encoded = self.tokenizer(
                    target_output,
                    add_special_tokens=False,
                    return_tensors='pt'
                )
                output_ids = output_encoded['input_ids'].to(self.device)
                output_emb = self.model.model.embed_tokens(output_ids)
                output_mask = output_encoded['attention_mask'].to(self.device)
                all_embeddings.append(output_emb)
                all_masks.append(output_mask)
                all_token_ids.extend(output_ids[0].tolist())

                # <|im_end|>
                all_embeddings.append(im_end_emb)
                all_masks.append(torch.ones((1, 1), device=self.device))
                all_token_ids.append(im_end_token)

            # logger.debug(f"ğŸ” Outputå¼€å§‹ä½ç½®: {output_start_pos}")

        # 5. æ‹¼æ¥æ‰€æœ‰embeddings
        # [1, total_seq_len, embed_dim]
        combined_embeddings = torch.cat(all_embeddings, dim=1)
        combined_attention_mask = torch.cat(
            all_masks, dim=1)   # [1, total_seq_len]

        # logger.debug(f"ğŸ” æ‹¼æ¥åembeddingså½¢çŠ¶: {combined_embeddings.shape}")

        # 6. æ ¹æ®æ¨¡å¼è¿›è¡Œä¸åŒçš„å¤„ç†
        max_length = getattr(self, 'max_length', 1024)
        seq_len = combined_embeddings.size(1)

        # åˆ¤æ–­æ˜¯å¦æ˜¯æ¨ç†æ¨¡å¼
        is_inference_mode = (target_output == "INFERENCE_MODE")

        if is_inference_mode:
            # æ¨ç†æ¨¡å¼ï¼šä¸éœ€è¦paddingï¼Œåªéœ€è¦æˆªæ–­è¿‡é•¿åºåˆ—
            if seq_len > max_length:
                logger.warning(
                    f"âš ï¸ æ¨ç†åºåˆ—é•¿åº¦ {seq_len} è¶…è¿‡max_length {max_length}ï¼Œè¿›è¡Œæˆªæ–­")
                combined_embeddings = combined_embeddings[:, :max_length, :]
                combined_attention_mask = combined_attention_mask[:, :max_length]
            # æ¨ç†æ¨¡å¼ä¸éœ€è¦labels
            labels = None
        else:
            # è®­ç»ƒæ¨¡å¼ï¼šéœ€è¦æ„å»ºlabelså¹¶è¿›è¡Œpaddingä»¥ä¿è¯æ‰¹å¤„ç†ä¸€è‡´æ€§
            # ç¡®ä¿token_idsé•¿åº¦ä¸embeddingsä¸€è‡´
            if len(all_token_ids) != seq_len:
                logger.warning(
                    f"âš ï¸ Token IDsé•¿åº¦ ({len(all_token_ids)}) ä¸embeddingsé•¿åº¦ ({seq_len}) ä¸åŒ¹é…")
                if len(all_token_ids) > seq_len:
                    all_token_ids = all_token_ids[:seq_len]
                else:
                    all_token_ids.extend([0] * (seq_len - len(all_token_ids)))

            if seq_len > max_length:
                # æˆªæ–­ï¼šåŒæ­¥å¤„ç†embeddings, attention_mask, token_ids, output_start_pos
                combined_embeddings = combined_embeddings[:, :max_length, :]
                combined_attention_mask = combined_attention_mask[:, :max_length]
                all_token_ids = all_token_ids[:max_length]

                # å¦‚æœoutput_start_posè¢«æˆªæ–­äº†ï¼Œéœ€è¦è°ƒæ•´
                if output_start_pos is not None and output_start_pos >= max_length:
                    logger.warning(
                        f"âš ï¸ Outputå¼€å§‹ä½ç½® {output_start_pos} è¶…è¿‡max_length {max_length}ï¼Œè°ƒæ•´ä¸ºNone")
                    output_start_pos = None

            elif seq_len < max_length:
                # è®­ç»ƒæ¨¡å¼éœ€è¦Paddingï¼šåŒæ­¥å¤„ç†embeddings, attention_mask, token_ids
                pad_length = max_length - seq_len
                embed_dim = combined_embeddings.size(2)

                # Padding embeddings (ç”¨0å¡«å……)
                pad_embeddings = torch.zeros(
                    (1, pad_length, embed_dim), device=self.device)
                combined_embeddings = torch.cat(
                    [combined_embeddings, pad_embeddings], dim=1)

                # Padding attention mask (ç”¨0å¡«å……)
                pad_mask = torch.zeros((1, pad_length), device=self.device)
                combined_attention_mask = torch.cat(
                    [combined_attention_mask, pad_mask], dim=1)

                # Padding token IDs (ç”¨0å¡«å……)
                all_token_ids.extend([0] * pad_length)

            # æ„å»ºlabels tensor (ä»…è®­ç»ƒæ¨¡å¼)
            labels = torch.tensor([all_token_ids], device=self.device)
            assert labels.size(1) == combined_embeddings.size(
                1), f"Labelsé•¿åº¦ {labels.size(1)} ä¸embeddingsé•¿åº¦ {combined_embeddings.size(1)} ä¸åŒ¹é…"

        # logger.debug(f"ğŸ” æœ€ç»ˆembeddingså½¢çŠ¶: {combined_embeddings.shape}")
        # if output_start_pos is not None:
        #     logger.debug(f"ğŸ” SFTæ¨¡å¼: åªè®¡ç®—ä½ç½® {output_start_pos} ä¹‹åçš„æŸå¤±")

        return combined_embeddings, combined_attention_mask, output_start_pos, labels

    def forward_with_embeddings(self,
                                embeddings: torch.Tensor,
                                attention_mask: torch.Tensor,
                                labels: Optional[torch.Tensor] = None,
                                output_start_pos: Optional[int] = None) -> Dict[str, torch.Tensor]:
        """
        ä½¿ç”¨é¢„è®¡ç®—çš„embeddingsè¿›è¡Œå‰å‘ä¼ æ’­

        Args:
            embeddings: è¾“å…¥embeddings [batch_size, seq_len, embedding_dim]
            attention_mask: æ³¨æ„åŠ›æ©ç  [batch_size, seq_len]
            labels: æ ‡ç­¾token IDs [batch_size, seq_len]
            output_start_pos: outputéƒ¨åˆ†å¼€å§‹çš„ä½ç½®ï¼ˆç”¨äºSFTåªè®¡ç®—outputéƒ¨åˆ†çš„lossï¼‰

        Returns:
            è¾“å‡ºå­—å…¸åŒ…å«logitså’Œå¯é€‰çš„loss
        """
        batch_size, seq_len = embeddings.shape[:2]
        position_ids = torch.arange(
            seq_len, device=self.device).unsqueeze(0).repeat(batch_size, 1)

        # ç›´æ¥ä½¿ç”¨Qwen2Modelçš„transformerå±‚
        outputs = self.model.model(
            inputs_embeds=embeddings,
            attention_mask=attention_mask,
            position_ids=position_ids,
            return_dict=True
        )

        hidden_states = outputs.last_hidden_state
        logits = self.model.lm_head(hidden_states)

        outputs = {"logits": logits}

        # SFTæŸå¤±è®¡ç®—ï¼šåªè®¡ç®—outputéƒ¨åˆ†çš„æŸå¤±
        if labels is not None:
            # [batch_size, seq_len-1, vocab_size]
            shift_logits = logits[..., :-1, :].contiguous()
            # [batch_size, seq_len-1]
            shift_labels = labels[..., 1:].contiguous()

            # å¦‚æœæŒ‡å®šäº†output_start_posï¼Œåªè®¡ç®—outputéƒ¨åˆ†çš„æŸå¤±
            if output_start_pos is not None:
                # æ³¨æ„ï¼šç”±äºshiftæ“ä½œï¼Œoutput_start_poséœ€è¦å‡1
                # åŸå› ï¼šshift_labelså»æ‰äº†ç¬¬ä¸€ä¸ªtokenï¼Œæ‰€ä»¥æ‰€æœ‰ä½ç½®éƒ½å‘å‰ç§»åŠ¨äº†1
                adjusted_output_start_pos = max(0, output_start_pos - 1)

                # åˆ›å»ºloss maskï¼šåªå¯¹outputéƒ¨åˆ†è®¡ç®—æŸå¤±
                loss_mask = torch.zeros_like(shift_labels, dtype=torch.bool)
                # åªæœ‰outputéƒ¨åˆ†ä¸ºTrue
                loss_mask[:, adjusted_output_start_pos:] = True

                # åªé€‰æ‹©outputéƒ¨åˆ†çš„logitså’Œlabels
                # [num_output_tokens, vocab_size]
                masked_logits = shift_logits[loss_mask]
                masked_labels = shift_labels[loss_mask]  # [num_output_tokens]

                if masked_logits.numel() > 0:  # ç¡®ä¿æœ‰è¾“å‡ºtokens
                    loss_fct = nn.CrossEntropyLoss()
                    loss = loss_fct(masked_logits, masked_labels)
                else:
                    loss = torch.tensor(
                        0.0, device=self.device, requires_grad=True)

                # logger.debug(f"ğŸ” SFTæŸå¤±è®¡ç®—: åŸå§‹output_start_pos={output_start_pos}, è°ƒæ•´å={adjusted_output_start_pos}, è¾“å‡ºtokensæ•°é‡={masked_logits.size(0) if masked_logits.numel() > 0 else 0}")
            else:
                # æ ‡å‡†çš„å…¨åºåˆ—æŸå¤±è®¡ç®—
                loss_fct = nn.CrossEntropyLoss()
                loss = loss_fct(
                    shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
                # logger.debug("ğŸ” ä½¿ç”¨å…¨åºåˆ—æŸå¤±è®¡ç®—")

            outputs["loss"] = loss

        return outputs

    def generate_with_embeddings(self,
                                 embeddings: torch.Tensor,
                                 attention_mask: torch.Tensor,
                                 max_new_tokens: int = 100,
                                 temperature: float = 0.7,
                                 do_sample: bool = True) -> str:
        """
        ä½¿ç”¨embeddingsç”Ÿæˆæ–‡æœ¬ - æ”¹è¿›ç‰ˆæœ¬

        æ³¨æ„ï¼šç”±äºHugging Faceçš„generateæ–¹æ³•ä¸æ”¯æŒinputs_embedsï¼Œ
        æˆ‘ä»¬éœ€è¦æ‰‹åŠ¨å®ç°generation loopï¼Œä½†è¿™æ¯”è¾ƒä½æ•ˆã€‚
        æ›´å¥½çš„æ–¹æ³•æ˜¯å°†embeddingsè½¬å›input_idsï¼Œä½†åœ¨Split Learningä¸­è¿™ä¸å¯è¡Œã€‚
        """
        try:
            with torch.no_grad():
                batch_size, seq_len = embeddings.shape[:2]
                # logger.debug(f"ğŸ” ç”Ÿæˆè°ƒè¯•: åˆå§‹embeddingså½¢çŠ¶={embeddings.shape}")

                generated_tokens = []
                current_embeddings = embeddings.clone()
                current_mask = attention_mask.clone()

                # è·å–åˆå§‹åºåˆ—é•¿åº¦ï¼Œç”¨äºposition_idsè®¡ç®—
                initial_seq_len = seq_len

                for step in range(max_new_tokens):
                    # è®¡ç®—æ­£ç¡®çš„position_ids
                    current_seq_len = current_embeddings.size(1)
                    position_ids = torch.arange(
                        current_seq_len, device=self.device).unsqueeze(0)

                    # ç›´æ¥ä½¿ç”¨Qwenæ¨¡å‹è¿›è¡Œå‰å‘ä¼ æ’­ï¼ˆé¿å…é‡å¤çš„position_idsè®¡ç®—ï¼‰
                    transformer_outputs = self.model.model(
                        inputs_embeds=current_embeddings,
                        attention_mask=current_mask,
                        position_ids=position_ids,
                        return_dict=True
                    )

                    hidden_states = transformer_outputs.last_hidden_state
                    logits = self.model.lm_head(hidden_states)

                    # è·å–æœ€åä¸€ä¸ªä½ç½®çš„logits
                    last_logits = logits[0, -1, :]  # [vocab_size]

                    # é‡‡æ ·ä¸‹ä¸€ä¸ªtoken
                    if do_sample and temperature > 0:
                        last_logits = last_logits / temperature
                        probs = torch.softmax(last_logits, dim=-1)
                        next_token_id = torch.multinomial(probs, num_samples=1)
                    else:
                        next_token_id = torch.argmax(
                            last_logits, dim=-1, keepdim=True)

                    token_id = next_token_id.item()
                    generated_tokens.append(token_id)

                    # æ£€æŸ¥ç»“æŸæ¡ä»¶
                    if token_id == self.tokenizer.eos_token_id:
                        # logger.debug(f"ğŸ” é‡åˆ°EOS tokenï¼Œåœæ­¢ç”Ÿæˆ")
                        break

                    # æ·»åŠ æ–°tokençš„embedding
                    next_token_embedding = self.model.model.embed_tokens(
                        next_token_id.unsqueeze(0))
                    current_embeddings = torch.cat(
                        [current_embeddings, next_token_embedding], dim=1)
                    current_mask = torch.cat(
                        [current_mask, torch.ones((1, 1), device=self.device)], dim=1)

                    # é˜²æ­¢åºåˆ—è¿‡é•¿
                    if current_embeddings.size(1) > 4096:
                        logger.warning("åºåˆ—é•¿åº¦è¶…è¿‡4096ï¼Œåœæ­¢ç”Ÿæˆ")
                        break

                # è°ƒè¯•ä¿¡æ¯
                # logger.debug(f"ğŸ” ç”Ÿæˆå®Œæˆ: å…±ç”Ÿæˆ{len(generated_tokens)}ä¸ªtokens")
                # logger.debug(f"ğŸ” ç”Ÿæˆçš„token ids: {generated_tokens[:10]}...")  # åªæ˜¾ç¤ºå‰10ä¸ª

                # è§£ç ç”Ÿæˆçš„tokens
                if generated_tokens:
                    # å…ˆå°è¯•ä¸è·³è¿‡ç‰¹æ®Štokençš„è§£ç 
                    generated_text = self.tokenizer.decode(
                        generated_tokens, skip_special_tokens=False)
                    # logger.debug(f"ğŸ” è§£ç ç»“æœ(å«ç‰¹æ®Štoken)é•¿åº¦: {len(generated_text)}, å†…å®¹: '{generated_text[:100]}'")

                    # å¦‚æœåŒ…å«ç‰¹æ®Štokenï¼Œå†å°è¯•è·³è¿‡ç‰¹æ®Štokençš„è§£ç 
                    generated_text_clean = self.tokenizer.decode(
                        generated_tokens, skip_special_tokens=True)
                    # logger.debug(f"ğŸ” è§£ç ç»“æœ(è·³è¿‡ç‰¹æ®Štoken)é•¿åº¦: {len(generated_text_clean)}, å†…å®¹: '{generated_text_clean[:100]}'")

                    # å¦‚æœè·³è¿‡ç‰¹æ®Štokenåä¸ºç©ºï¼Œè¿”å›åŸå§‹è§£ç ç»“æœ
                    if generated_text_clean.strip():
                        return generated_text_clean.strip()
                    else:
                        # ç§»é™¤å¸¸è§çš„ç‰¹æ®Štokenï¼Œä½†ä¿ç•™æœ‰æ„ä¹‰çš„æ–‡æœ¬
                        filtered_text = generated_text.replace('<|endoftext|>', '').replace(
                            '<|im_start|>', '').replace('<|im_end|>', '')
                        # logger.debug(f"ğŸ” æ‰‹åŠ¨è¿‡æ»¤åç»“æœ: '{filtered_text[:100]}'")
                        return filtered_text.strip()
                else:
                    # logger.debug(f"ğŸ” æ²¡æœ‰ç”Ÿæˆä»»ä½•token")
                    return ""

        except Exception as e:
            logger.error(f"ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return "ç”Ÿæˆå¤±è´¥"

    def to(self, device):
        """ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡"""
        super().to(device)
        self.device = device
        return self


class FederatedQwenSystem(nn.Module):
    """
    Split Learningè”é‚¦Qwenç³»ç»Ÿ

    æ¶æ„è®¾è®¡ï¼š
    1. å®¢æˆ·ç«¯åªä¿ç•™embeddingå±‚ï¼Œè¿›è¡Œå‰å‘ä¼ æ’­
    2. æœåŠ¡ç«¯æ¥æ”¶å®¢æˆ·ç«¯embeddingsï¼Œè¿›è¡Œå®Œæ•´çš„å‰å‘å’Œåå‘ä¼ æ’­
    3. æœåŠ¡ç«¯æ›´æ–°embeddingæƒé‡ååˆ†å‘ç»™æ‰€æœ‰å®¢æˆ·ç«¯
    4. å®¢æˆ·ç«¯ä¸å‚ä¸åå‘ä¼ æ’­ï¼Œåªè´Ÿè´£embeddingè®¡ç®—
    """

    def __init__(self,
                 model_path: str = "/root/autodl-tmp/Federated_learning/llm_init_ckpt/Qwen2.5-0.5B-Instruct",
                 device: str = 'cuda',
                 enabled_clients: List[str] = None):
        super().__init__()

        self.model_path = model_path
        self.device = device

        # é»˜è®¤å¯ç”¨æ‰€æœ‰å®¢æˆ·ç«¯
        if enabled_clients is None:
            enabled_clients = ['port', 'railway', 'customs']
        self.enabled_clients = enabled_clients

        # åˆå§‹åŒ–å¯ç”¨çš„å®¢æˆ·ç«¯embeddings
        self.client_embeddings = nn.ModuleDict()
        for client_name in enabled_clients:
            if client_name in ['port', 'railway', 'customs']:
                self.client_embeddings[client_name] = QwenClientEmbedding(
                    client_name, model_path, device=device
                )
            else:
                logger.warning(f"æœªçŸ¥çš„å®¢æˆ·ç«¯ç±»å‹: {client_name}")

        logger.info(f"âœ… åˆå§‹åŒ–è”é‚¦ç³»ç»Ÿï¼Œå¯ç”¨å®¢æˆ·ç«¯: {list(self.client_embeddings.keys())}")

        # åˆå§‹åŒ–æœåŠ¡ç«¯æ¨¡å‹
        self.server_model = QwenServerModel(
            model_path=model_path, device=device)

        # å…±äº«tokenizer
        self.tokenizer = self.server_model.tokenizer

        # ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
        self.to(device)

        # ğŸ”„ å…³é”®ï¼šåˆå§‹åŒæ­¥æœåŠ¡ç«¯embeddingåˆ°å®¢æˆ·ç«¯ï¼ˆåœ¨è®¾å¤‡ç§»åŠ¨åï¼‰
        self._sync_embeddings_to_clients()

        logger.info("âœ… è”é‚¦Qwenç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼ˆå·²åŒæ­¥embeddingï¼‰")

    def _sync_embeddings_to_clients(self):
        """
        ğŸ”„ Split Learningæ ¸å¿ƒæœºåˆ¶ï¼šå°†æœåŠ¡ç«¯embeddingæƒé‡åŒæ­¥åˆ°æ‰€æœ‰å®¢æˆ·ç«¯

        Split Learningçš„å…³é”®æ­¥éª¤ï¼š
        1. æœåŠ¡ç«¯é€šè¿‡åå‘ä¼ æ’­æ›´æ–°embeddingæƒé‡
        2. å°†æ›´æ–°åçš„æƒé‡åˆ†å‘ç»™æ‰€æœ‰å®¢æˆ·ç«¯
        3. å®¢æˆ·ç«¯æ¥æ”¶æ–°æƒé‡ï¼Œç”¨äºä¸‹ä¸€è½®å‰å‘ä¼ æ’­
        4. å®¢æˆ·ç«¯ä¸è¿›è¡Œåå‘ä¼ æ’­ï¼Œåªè´Ÿè´£embeddingè®¡ç®—
        """
        # logger.info("ğŸ”„ åŒæ­¥æœåŠ¡ç«¯embeddingæƒé‡åˆ°å®¢æˆ·ç«¯...")

        # è·å–æœåŠ¡ç«¯çš„embeddingæƒé‡ (è¿™æ˜¯ground truth)
        server_embed_weight = self.server_model.model.model.embed_tokens.weight.data

        # åŒæ­¥åˆ°æ¯ä¸ªå®¢æˆ·ç«¯
        sync_count = 0
        for client_name, client_embedding in self.client_embeddings.items():
            with torch.no_grad():
                # ç›´æ¥å¤åˆ¶æœåŠ¡ç«¯æƒé‡åˆ°å®¢æˆ·ç«¯
                client_embedding.embed_tokens.weight.copy_(server_embed_weight)
            sync_count += 1
            # logger.debug(f"   âœ… {client_name}å®¢æˆ·ç«¯embeddingå·²åŒæ­¥")

        # logger.info(f"âœ… æˆåŠŸåŒæ­¥embeddingæƒé‡åˆ°{sync_count}ä¸ªå®¢æˆ·ç«¯")

    def federated_step(self):
        """
        ğŸ”„ æ‰§è¡ŒSplit Learningçš„æƒé‡åŒæ­¥æ­¥éª¤

        Split Learningæµç¨‹ï¼š
        1. æœåŠ¡ç«¯å·²é€šè¿‡åå‘ä¼ æ’­æ›´æ–°embeddingæƒé‡
        2. å°†æ›´æ–°åçš„æƒé‡åˆ†å‘ç»™æ‰€æœ‰å®¢æˆ·ç«¯
        3. å®¢æˆ·ç«¯æ¥æ”¶æ–°æƒé‡ï¼Œå‡†å¤‡ä¸‹ä¸€è½®å‰å‘ä¼ æ’­

        æ³¨æ„ï¼šå®¢æˆ·ç«¯ä¸å‚ä¸æ¢¯åº¦è®¡ç®—ï¼Œåªæ¥æ”¶æƒé‡æ›´æ–°
        è¿™ä¸ªæ–¹æ³•åº”è¯¥åœ¨æ¯ä¸ªè®­ç»ƒæ­¥éª¤çš„optimizer.step()ä¹‹åè°ƒç”¨
        """
        # Split Learning: åªéœ€è¦åŒæ­¥æƒé‡ï¼Œä¸éœ€è¦èšåˆæ¢¯åº¦
        # å› ä¸ºå®¢æˆ·ç«¯ä¸è®¡ç®—æ¢¯åº¦ï¼Œæ‰€æœ‰æ¢¯åº¦è®¡ç®—éƒ½åœ¨æœåŠ¡ç«¯å®Œæˆ
        self._sync_embeddings_to_clients()

        # logger.debug("ğŸ”„ Split Learningæƒé‡åŒæ­¥å®Œæˆ")

    def forward(self,
                server_instruction: str,
                client_instructions: Dict[str, str],
                labels: Optional[torch.Tensor] = None,
                target_output: str = None) -> Dict[str, torch.Tensor]:
        """
        Split Learningå‰å‘ä¼ æ’­æµç¨‹

        Decoder-only LLMçš„æ­£ç¡®è®­ç»ƒæµç¨‹ï¼š
        1. å®¢æˆ·ç«¯å„è‡ªè®¡ç®—embeddingï¼ˆä¸è®¡ç®—æ¢¯åº¦ï¼‰
        2. æœåŠ¡ç«¯æ‹¼æ¥ï¼šinput + outputï¼ˆå¦‚æœæ˜¯è®­ç»ƒï¼‰
        3. æœåŠ¡ç«¯è¿›è¡Œå®Œæ•´çš„å‰å‘ä¼ æ’­
        4. æŸå¤±åªåœ¨outputéƒ¨åˆ†è®¡ç®—

        Args:
            server_instruction: æœåŠ¡ç«¯æŒ‡ä»¤
            client_instructions: å®¢æˆ·ç«¯æŒ‡ä»¤å­—å…¸ {"port": "...", "railway": "..."}
            labels: æ ‡ç­¾å¼ é‡ï¼ˆç”¨äºè®¡ç®—æŸå¤±ï¼‰
            target_output: ç›®æ ‡è¾“å‡ºæ–‡æœ¬ï¼ˆè®­ç»ƒæ—¶å¿…é¡»æä¾›ï¼‰

        Returns:
            è¾“å‡ºå­—å…¸åŒ…å«logitså’Œloss
        """
        # 1. å®¢æˆ·ç«¯embeddingç¼–ç  - Split Learningæ–¹å¼
        client_embeddings = {}
        for client_name, instruction in client_instructions.items():
            # ç›´æ¥ä½¿ç”¨å®¢æˆ·ç«¯åç§°ï¼Œæ”¯æŒä¸¤ç§æ ¼å¼
            if client_name in self.client_embeddings:
                client_key = client_name
            elif '_' in client_name:
                # å¤„ç† "client_01" æ ¼å¼
                client_type = client_name.split('_')[1]
                if client_type == '01':
                    client_key = 'port'
                elif client_type == '02':
                    client_key = 'railway'
                elif client_type == '03':
                    client_key = 'customs'
                else:
                    client_key = 'port'  # é»˜è®¤
            else:
                client_key = 'port'  # é»˜è®¤

            # Split Learning: å®¢æˆ·ç«¯åªè¿›è¡Œå‰å‘ä¼ æ’­
            embeddings = self.client_embeddings[client_key](instruction)
            client_embeddings[client_name] = embeddings

        # 2. æœåŠ¡ç«¯æ‹¼æ¥å’Œå¤„ç†ï¼ˆæ­£ç¡®çš„decoder-onlyæ–¹å¼ï¼‰
        combined_embeddings, combined_mask, output_start_pos, labels = self.server_model.create_federated_input_embeddings(
            server_instruction, client_embeddings, target_output
        )

        # 3. æœåŠ¡ç«¯å‰å‘ä¼ æ’­
        if self.training and target_output is not None:
            # SFTè®­ç»ƒæ¨¡å¼ï¼šä½¿ç”¨å·²æ„å»ºçš„labelså’Œoutput_start_pos
            outputs = self.server_model.forward_with_embeddings(
                combined_embeddings, combined_mask, labels, output_start_pos
            )
        else:
            outputs = self.server_model.forward_with_embeddings(
                combined_embeddings, combined_mask, None
            )

        return outputs

    def generate(self,
                 server_instruction: str,
                 client_instructions: Dict[str, str],
                 max_new_tokens: int = 100,
                 temperature: float = 0.7,
                 do_sample: bool = True) -> str:
        """ç”Ÿæˆæ–‡æœ¬"""
        # å®¢æˆ·ç«¯embeddingç¼–ç 
        client_embeddings = {}
        for client_name, instruction in client_instructions.items():
            # æ ¹æ®å®¢æˆ·ç«¯åç§°ç¡®å®šç±»å‹
            if 'port' in client_name.lower():
                client_key = 'port'
            elif 'railway' in client_name.lower():
                client_key = 'railway'
            elif 'customs' in client_name.lower():
                client_key = 'customs'
            else:
                # å¦‚æœåç§°ä¸åŒ¹é…ï¼Œç›´æ¥ä½¿ç”¨åç§°
                if client_name in self.client_embeddings:
                    client_key = client_name
                else:
                    client_key = 'port'  # é»˜è®¤ä½¿ç”¨port

            embeddings = self.client_embeddings[client_key](instruction)
            client_embeddings[client_name] = embeddings

        # æœåŠ¡ç«¯æ‹¼æ¥ï¼ˆæ¨ç†æ—¶éœ€è¦æ·»åŠ assistantæç¤ºï¼‰
        combined_embeddings, combined_mask, _, _ = self.server_model.create_federated_input_embeddings(
            server_instruction, client_embeddings, target_output="INFERENCE_MODE"  # ç‰¹æ®Šæ ‡è¯†è¡¨ç¤ºæ¨ç†æ¨¡å¼
        )

        # ç”Ÿæˆ
        return self.server_model.generate_with_embeddings(
            combined_embeddings, combined_mask, max_new_tokens, temperature, do_sample
        )

    def to(self, device):
        """ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡"""
        super().to(device)
        self.device = device
        for client_emb in self.client_embeddings.values():
            client_emb.to(device)
        self.server_model.to(device)
        return self


# å¯¼å‡ºä¸»è¦ç±»
__all__ = ['QwenClientEmbedding', 'QwenServerModel', 'FederatedQwenSystem']
